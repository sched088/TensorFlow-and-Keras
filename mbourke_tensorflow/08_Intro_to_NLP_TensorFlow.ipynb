{"cells":[{"cell_type":"markdown","metadata":{"id":"gzknYZh96wcS"},"source":["# 8. Natural Language Processing\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-example-nlp-problems.png)\n","*A handful of example natural language processing (NLP) and natural language understanding (NLU) problems. These are also often referred to as sequence problems (going from one sequence to another).*\n","\n","Natural Language Processing (NLP) can cover many applications, including **text and speech**. Both of these types of data are known as *sequences* i.e. a sequence of words a common term in NLP is *seq2seq*, finding information from one sequence to produce another sequence. \n","\n","A couple of example applications are **spam filtering** (classification) or **analyzing customer complaints**.\n","\n","Many of the steps below will be similar to the previous modules, but applied to text data.\n","\n","```\n","Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n","```\n","\n","*Note: This notebook is part of a code-along creaded by Daniel Bourke and hosted on his [github](https://github.com/mrdbourke/tensorflow-deep-learning)*\n","\n","## In this notebook we will work with:\n","\n","* Downloading a text dataset\n","* Visualizing text data\n","* Converting text into numbers using tokenization\n","* Turning our tokenized text into an embedding\n","* Modelling a text dataset\n","  * Starting with a baseline (TF-IDF)\n","  * Building several deep learning text models\n","    * Dense, LSTM, GRU, Conv1D, Transfer learning\n","* Comparing the performance of each our models\n","* Combining our models into an ensemble\n","* Saving and loading a trained model\n","* Find the most wrong predictions"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658348439474,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"9cRztY1RB-Kv","outputId":"53d622db-fd5a-460d-8815-b83b600f512c"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-76ef2a01-401c-a9e6-df3d-5b280f59ee3e)\n"]}],"source":["# Check for GPU: Note I am not getting access to the GPU now because of how much I've been using it previously...\n","!nvidia-smi -L"]},{"cell_type":"markdown","metadata":{"id":"B-cZfV7ODGV1"},"source":["## Get our helper functions from previous modules"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1658348439768,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"GFl3tDGsDMGd","outputId":"d3072e1b-a92c-4c04-d79a-029100daad50"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-20 20:20:40--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10246 (10K) [text/plain]\n","Saving to: ‘helper_functions.py’\n","\n","helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n","\n","2022-07-20 20:20:40 (117 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n","\n"]}],"source":["# Download helper functions from github\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"F-jh9OPeDPVB","executionInfo":{"status":"ok","timestamp":1658348444507,"user_tz":300,"elapsed":4740,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Import relevant functions\n","from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"]},{"cell_type":"markdown","metadata":{"id":"ZCjVUVteDSTd"},"source":["## Download text dataset\n","\n","To start we will be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) datset from Kaggle which contains text-based Tweets about natural disasters. \n","\n","Overview of the dataset:\n","The Real Tweets are actually about diasters, for example:\n","\n","```\n","Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n","```\n","\n","The Not Real Tweets are Tweets not about diasters (they can be on anything), for example:\n","\n","```\n","'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n","```\n","\n","For convenience, the dataset has been [downloaded unchanged from Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) as to download from Kaggle you need a log-in"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1658348444727,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"a-GCAnjDDgpS","outputId":"ab8cc780-6121-4362-a13f-52547d38428c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-20 20:20:45--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.135.128, 74.125.142.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 607343 (593K) [application/zip]\n","Saving to: ‘nlp_getting_started.zip’\n","\n","\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.005s  \n","\n","2022-07-20 20:20:45 (118 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n","\n"]}],"source":["# Download data (same as from Kaggle)\n","!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n","\n","# Unzip data\n","unzip_data(\"nlp_getting_started.zip\")"]},{"cell_type":"markdown","metadata":{"id":"3uwLO8tDDmoH"},"source":["Unzipping `nlp_getting_started.zip` gives the following 3 `.csv` files:\n","* `sample_submission.csv` - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n","* `train.csv` - training samples of real and not real diaster Tweets.\n","* `test.csv` - testing samples of real and not real diaster Tweets."]},{"cell_type":"markdown","metadata":{"id":"jyj9itZvDz3t"},"source":["## Visualizing the text dataset\n","\n","Motto: **Visualize, Visualize, Visualize**\n","\n","Step 1 is to use Pandas DataFrames."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1658348444935,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"k-Tp0fH-Ed7T","outputId":"729ec2ca-5784-4b57-c51f-2b0046f050e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "],"text/html":["\n","  <div id=\"df-acb3c2b1-e6fe-4143-960d-44ce98695de8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acb3c2b1-e6fe-4143-960d-44ce98695de8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-acb3c2b1-e6fe-4143-960d-44ce98695de8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-acb3c2b1-e6fe-4143-960d-44ce98695de8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["# Turn .CSVs into pandas DataFrames\n","import pandas as pd\n","\n","train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","train_df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1658348445115,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"PeAk63ooEoO5","outputId":"8bd2e246-3120-4d1b-dacd-cbbaf3a8e927"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id      keyword               location  \\\n","2644  3796  destruction                    NaN   \n","2227  3185       deluge                    NaN   \n","5448  7769       police                     UK   \n","132    191   aftershock                    NaN   \n","6845  9810       trauma  Montgomery County, MD   \n","\n","                                                   text  target  \n","2644  So you have a new weapon that can cause un-ima...       1  \n","2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n","5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n","132   Aftershock back to school kick off was great. ...       0  \n","6845  in response to trauma Children of Addicts deve...       0  "],"text/html":["\n","  <div id=\"df-e71db976-e5c3-4051-87ea-c3317cd6a9bd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2644</th>\n","      <td>3796</td>\n","      <td>destruction</td>\n","      <td>NaN</td>\n","      <td>So you have a new weapon that can cause un-ima...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2227</th>\n","      <td>3185</td>\n","      <td>deluge</td>\n","      <td>NaN</td>\n","      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5448</th>\n","      <td>7769</td>\n","      <td>police</td>\n","      <td>UK</td>\n","      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>191</td>\n","      <td>aftershock</td>\n","      <td>NaN</td>\n","      <td>Aftershock back to school kick off was great. ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6845</th>\n","      <td>9810</td>\n","      <td>trauma</td>\n","      <td>Montgomery County, MD</td>\n","      <td>in response to trauma Children of Addicts deve...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e71db976-e5c3-4051-87ea-c3317cd6a9bd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e71db976-e5c3-4051-87ea-c3317cd6a9bd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e71db976-e5c3-4051-87ea-c3317cd6a9bd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["# Shuffle data to be sure it's shuffled\n","train_df_shuffled = train_df.sample(frac=1, random_state=42) # set random to 42 for reproducibility\n","train_df_shuffled.head()"]},{"cell_type":"markdown","metadata":{"id":"q8CRQtaoE-3a"},"source":["The `target` column does not exist in the test dataset. This is what we want to predict using the `text` column in the training dataset\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png)\n","*Example text classification inputs and outputs for the problem of classifying whether a Tweet is about a diaster or not.*"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1658348445118,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"RfnqAB4nFNeq","outputId":"127870c4-6572-40b2-811f-e13e9e053547"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id keyword location                                               text\n","0   0     NaN      NaN                 Just happened a terrible car crash\n","1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n","2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n","3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n","4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"],"text/html":["\n","  <div id=\"df-5b0119c7-ef88-499c-a61c-52e2ce843991\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b0119c7-ef88-499c-a61c-52e2ce843991')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5b0119c7-ef88-499c-a61c-52e2ce843991 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5b0119c7-ef88-499c-a61c-52e2ce843991');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["# Look at test_df\n","test_df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1658348445119,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"-_Aj25QNLMp8","outputId":"edc5fd4a-c0e4-4aea-db24-caf64271c9a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    4342\n","1    3271\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":8}],"source":["# How many samples of each class\n","train_df.target.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"A8WA1OVZLZnw"},"source":["Fairly balanced dataset with 60% negative class (target = 0) and 40% positive class (target = 1)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1658348445121,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"oaX4P1whLo-Q","outputId":"e1382c27-91e9-4121-8625-723783dc8ae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total training samples: 7613\n","Total test samples: 3263\n","Total samples: 10876\n"]}],"source":["# Counts of samples\n","print(f\"Total training samples: {len(train_df)}\")\n","print(f\"Total test samples: {len(test_df)}\")\n","print(f\"Total samples: {len(train_df) + len(test_df)}\")"]},{"cell_type":"markdown","metadata":{"id":"_KQLRYalLuVh"},"source":["For this dataset we can see that we have plenty of test data. Normally it's a 90/10 split, or 80/20, but here it's almost 50%!\n","\n","As we did with images previously, let's view some random text samples"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1658348445122,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"leEf-_ALMmNc","outputId":"759896f8-2cf3-4bb4-8d60-aff59c508171"},"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 0 (not real disaster)\n","Text:\n","@czallstarwes more like demolition derby ??\n","\n","--------\n","\n","Target: 0 (not real disaster)\n","Text:\n","Souda leave Lady Sonia alone or I shall obliterate you #KneelBot\n","\n","--------\n","\n","Target: 0 (not real disaster)\n","Text:\n","'Sometimes God uses sorrowful tragedy to set the stage for glorious redemption.' -David Platt Run forÛ_ https://t.co/86V81dv00E\n","\n","--------\n","\n","Target: 1 (real distaster)\n","Text:\n","IJ: Texas Seeks Comment on Rules for Changes to Windstorm Insurer http://t.co/h132iuL7MU\n","\n","--------\n","\n","Target: 0 (not real disaster)\n","Text:\n","School Of Seven Bells - Windstorm  http://t.co/E1kbluDwh5 #nowplaying\n","\n","--------\n","\n"]}],"source":["# Visualize some random training examples\n","import random\n","\n","random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n","for row in train_df_shuffled[['text', 'target']][random_index:random_index+5].itertuples():\n","  _, text, target = row\n","  print(f\"Target: {target}\", \"(real distaster)\" if target > 0 else \"(not real disaster)\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"--------\\n\")"]},{"cell_type":"markdown","metadata":{"id":"DzDVTj9HNPsS"},"source":["## Split data into trainin and validation datasets\n","\n","Since the test set has no labels and we need a way to evaluate our trained models, we'll split off soime of the training data and create a validation set. When our model trains it'll only see data from the training set and we can see how it performs on unseen data using the validation set.\n","\n","Additionally, we will convert the splits from pandas Series datatypes into lists of strings (for the text) and lists of ints (for the labels) for ease of use later. \n","\n","We will split using Scikit-Learn's `train_test_split()` method, and set 10% of the trainng data aside as validation data."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"H5zsFufbN7W0","executionInfo":{"status":"ok","timestamp":1658348445123,"user_tz":300,"elapsed":29,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the train data into train and validation\n","train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n","                                                                            train_df_shuffled[\"target\"].to_numpy(),\n","                                                                            test_size=0.1,\n","                                                                            random_state=42)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1658348445123,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"7KRM6REgOPHE","outputId":"af86bedc-7625-4e81-b064-cfc49bdc270f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6851, 6851, 762, 762)"]},"metadata":{},"execution_count":12}],"source":["# Check lengths (personally I would do this in %s too)\n","len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1658348445265,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"XbsEeup-O9Un","outputId":"70d5e516-9726-4d02-ede8-5373fe331b75"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n","        'Imagine getting flattened by Kurt Zouma',\n","        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n","        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n","        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n","        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n","        'destroy the free fandom honestly',\n","        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n","        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n","        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n","       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"]},"metadata":{},"execution_count":13}],"source":["# View the first 10 training sentences and their labels\n","train_sentences[:10], train_labels[:10]"]},{"cell_type":"markdown","metadata":{"id":"BXbmmnaEPCwR"},"source":["## Converting text into numbers\n","\n","Machine Learning only works well on numbers! In NLP there are two main concepts for doing this conversion.\n","1. **Tokenization** A direct mapping from word or character or sub-word to a numerical value. There are 3 main ways of doing this:\n"," \n"," * **Word-level tokenization**: Each word is given a numerical value\n"," * **Character-level tokenization**: Each character is given a numerical value\n"," * **Sub-word tokenization**: between the two above types of tokenization. More common on audio. e.g. \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\".\n","\n","The level of tokenization depends on the specific problem. You can compare in testing/experimenting and see which perform best. It is also possible to stack them (e.g. combining the outputs of your embedding layers using [`tf.keras.layers.concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)). \n","\n","2. **Embeddings**: An embedding is a representation of a natural language that can be learned. Representation comes in the form of a *feature vector*. For example the word 'heads' may be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. There are two ways to implement embeddings\n","\n"," * **Create your own embedding** once your text has been turned into numbers (unclear: tokenized?) it can be put through an embedding layer such as `tf.keras.layers.Embedding` and the embedding representation will be learned during training\n"," * **Reuse a prelearned embedding** Same as with pre-trained image model weights. These are generally available online and trained on large sets of data i.e. all of Wikipedia. We can use these and then fine-tune them.  \n","\n","Locations for finding pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding).\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n","*Example of **tokenization** (straight mapping from word to number) and **embedding** (richer representation of relationships between tokens).*"]},{"cell_type":"markdown","metadata":{"id":"uj5Bpbz2Q_NS"},"source":["## Text Vectorization\n","\n","First we will learn how to apply tokenization, then we will look at embedding.\n","\n","Tokenization: We will use the layer `tf.keras.experimental.preprocessing.TextVectorization`\n","\n","Parameters for the above layer:\n","* `max_tokens` - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens. \n","* `standardize` - Method for standardizing text. Default is `\"lower_and_strip_punctuation\"` which lowers text and removes all punctuation marks.\n","* `split` - How to split text, default is `\"whitespace\"` which splits on spaces.\n","* `ngrams` - How many words to contain per token split, for example, `ngrams=2` splits tokens into continuous sequences of 2.\n","* `output_mode` -  How to output tokens, can be `\"int\"` (integer mapping), `\"binary\"` (one-hot encoding), `\"count\"` or `\"tf-idf\"`. See documentation for more.\n","* `output_sequence_length` - Length of tokenized sequence to output. For example, if `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.\n","* `pad_to_max_tokens` - Defaults to `False`, if `True`, the output feature axis will be padded to `max_tokens` even if the number of unique tokens in the vocabulary is less than `max_tokens`. Only valid in certain modes, see docs for more."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"8Wqp_PX_RWlL","executionInfo":{"status":"ok","timestamp":1658348449725,"user_tz":300,"elapsed":4462,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","# We will use the default TextVectorization variables\n","text_vectorizer = TextVectorization(max_tokens=None, # max  words in the vocabulary (None = all words)\n","                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n","                                    split=\"whitespace\", # where to split\n","                                    ngrams=None, # create groups of k-words?\n","                                    output_mode=\"int\", # how to map tokens to numbers (vs float)\n","                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n","                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"]},{"cell_type":"markdown","metadata":{"id":"ZRkRQIytUVi0"},"source":["The above cell initializes a `TextVectorization` object with the default settings, but we would like to customize it for our specific case. \n","\n","1. `max_tokens`: It is common for this to be in multiples of 10,000, or the exact number of unique words in your corpus. We will use `10,000`.\n","2. `output_sequence_length`: We will use the average number of tokens per Tweet in the training set (which we still need to find). "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658348449726,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"wNpGXTYWvYSc","outputId":"52a1e6e4-fe1e-4001-c26d-85b4c17c72fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":15}],"source":["# Find the average number of tokens (words) in the training Tweets\n","round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"]},{"cell_type":"markdown","metadata":{"id":"UV2dkXhtvsy8"},"source":["Now we will be adding our custom settings to the `TextVectorization`."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Brg5JT69wIDs","executionInfo":{"status":"ok","timestamp":1658348449726,"user_tz":300,"elapsed":3,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Initialize text vecotrization with custom variables\n","max_vocab_length = 10000\n","max_length = 15 # Max length our sequences will be\n","\n","text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n","                                    output_mode=\"int\",\n","                                    output_sequence_length=max_length)"]},{"cell_type":"markdown","metadata":{"id":"N-LLkRXJwmjF"},"source":["This `TextVectorization` will be mapped to our data by calling the `adapt()` method while passing it to our training data."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"JcA4lK_SJNGG","executionInfo":{"status":"ok","timestamp":1658348451283,"user_tz":300,"elapsed":1560,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Fit the text vectorizer to the training text\n","text_vectorizer.adapt(train_sentences)"]},{"cell_type":"markdown","metadata":{"id":"3kNcT7FKJatq"},"source":["Now that this is mapped, let's send it an example sentence to see what is looks like."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1658348451639,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"OLeT7VN6JjQM","outputId":"cbbe0afc-c822-4e34-9665-97b6cf865c11"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n","array([[  8, 160,   3,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0]])>"]},"metadata":{},"execution_count":18}],"source":["sample_text = \"I am a banana\"\n","text_vectorizer([sample_text])"]},{"cell_type":"markdown","metadata":{"id":"EbIAoay8JqPO"},"source":["As we can see our output fills to a vector of length 15. While this is okay with padding short sentences, I wonder how this will impact Tweets longer than 15 tokens (words)...\n","\n","Let's try this on some random sentences from our trainin data."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658348451640,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"mNVNfjAIJ9AO","outputId":"6c6dd843-38e6-49be-cf54-174b9414d438"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n","Tell him Rebahe's going to destroy himself @Zenande_Mcfen @NDzedze #Ashes2Ashes      \n","\n","Vectorized version:\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n","array([[ 639,  158, 9305,  104,    5,  305, 1661, 6500,    1,    1,    0,\n","           0,    0,    0,    0]])>"]},"metadata":{},"execution_count":19}],"source":["# Select a random sentence from our training dataset and tokenize\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\nVectorized version:\")\n","text_vectorizer([random_sentence])"]},{"cell_type":"markdown","metadata":{"id":"-HBuoGekK1Ny"},"source":["Lastly, let's check our unique tokens in our vocabulary using `get_vocabulary()`"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1658348451873,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"uxDKSghmLW8C","outputId":"efa320f4-d751-4be2-ee90-ab1cc57401dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in vocab: 10000\n","Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n","Bottom 5 least common words: ['painthey', 'painful', 'paine', 'paging', 'pageshi', 'pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"]}],"source":["# Get the unique words in the vocabulary\n","words_in_vocab = text_vectorizer.get_vocabulary()\n","top_5_words = words_in_vocab[:10] # most common tokens\n","bottom_5_words = words_in_vocab[-10:]\n","\n","print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\") \n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"]},{"cell_type":"markdown","metadata":{"id":"-Hpfvl-aLsBG"},"source":["## Creating an Embedding using an Embedding Layer\n","\n","Okay, so we have created a mapping to map our tokens (words) to values, which we use to create vectors for our Tweets.\n","\n","What about now turning those values into embeddings? \n","\n","*Why?*:  Embeddings are powerful because they can be learned during training. This means that instead of being statis, a word's representation can be improved as a model goes through data samples (From my understanding embeddings will place similar words closer to each other in the embedding space).\n","\n","Let's use a `tf.keras.layers.Embedding` layer to see what the embedding of a word looks like. \n","\n","Parameters for `tf.keras.layers.Embedding`:\n","* `input_dim`: The size of the vocabulary. (i.e. `len(text_vectorizer.get_vocabulary()`.\n","* `output_dim`: The size of the output embedding vector.\n","* `embeddings_initializer`: How to initialize the embeddings matrix. Default is `\"uniform\"` which randomly initializes embedding matrix with uniform distribution. This is adjusted when using pre-learned embeddings\n","* `input_length`: Length of sequences being passed to the embedding layer."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1658348451875,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"uqOKaax-UjOp","outputId":"c95a0d42-173d-42ee-b21d-7c6d30d9e553"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.layers.embeddings.Embedding at 0x7f2b8657bb90>"]},"metadata":{},"execution_count":21}],"source":["# Create an embedding layer\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","\n","embedding = layers.Embedding(input_dim=max_vocab_length, # input shape\n","                             output_dim=128, # size of embedding vector\n","                             embeddings_initializer=\"uniform\", # initialize randomly\n","                             input_length=max_length, # length of input\n","                             name=\"embedding_1\")\n","embedding"]},{"cell_type":"markdown","metadata":{"id":"VaWBOfK8Vpep"},"source":["This output is significant because we can see that the `embedding` is a *Tensorflow layer*.\n","\n","This means that we can use it as part of the model, and let its parameters (word representations) be updated as the model learns.\n","\n","Let's look at a sample sentence"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1658348452155,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"2MuRuZ9xWVCU","outputId":"db879a1c-39c8-4124-d7bf-0cd12f3dd60a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n","Help yourself or those you love who suffer from self-esteem wounds. You can today! http://t.co/tu6ScRSXVG http://t.co/iDhj4JBQ05      \n","\\Embedded version:\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n","array([[[ 0.03740182,  0.03978038,  0.01312634, ...,  0.03317055,\n","          0.04473395,  0.02033745],\n","        [-0.0056263 , -0.03605131,  0.00469315, ...,  0.03750315,\n","         -0.02075065,  0.03189817],\n","        [-0.04538491,  0.02194316,  0.02299335, ...,  0.03143641,\n","         -0.00354359,  0.04591903],\n","        ...,\n","        [-0.01303345, -0.0456596 , -0.017446  , ..., -0.04159649,\n","          0.00482054, -0.00569761],\n","        [ 0.0291287 ,  0.02585142,  0.0255974 , ...,  0.00572053,\n","          0.02781173,  0.01084658],\n","        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n","          0.03332629,  0.02803668]]], dtype=float32)>"]},"metadata":{},"execution_count":22}],"source":["# Get random sentence\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\Embedded version:\")\n","\n","# Embed random sentence\n","sample_embed = embedding(text_vectorizer([random_sentence]))\n","sample_embed"]},{"cell_type":"markdown","metadata":{"id":"0xp0uq3rWuQO"},"source":["The embedding returns a 128 length feature vector as set by `output_dim`,,"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658348452155,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"_oEkKCDZXENW","outputId":"c9411fe7-bbe8-4533-810d-8e343aa06ff6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(128,), dtype=float32, numpy=\n","array([ 0.03740182,  0.03978038,  0.01312634,  0.04525057,  0.02647353,\n","       -0.01383851, -0.00156553, -0.00526694,  0.02261592,  0.01328261,\n","       -0.01635955, -0.02700753,  0.00973669,  0.03418609,  0.02739987,\n","        0.0141249 , -0.0327132 ,  0.01082782,  0.03346223, -0.02153643,\n","       -0.04562371,  0.02317332, -0.03271214,  0.02264846,  0.02845539,\n","       -0.00171893,  0.02275508,  0.00580122,  0.04756936,  0.03961797,\n","       -0.03917404, -0.0294854 ,  0.0359321 ,  0.02577246,  0.01569429,\n","       -0.03792466, -0.03001191,  0.01045021, -0.01078918, -0.00140404,\n","       -0.01669199,  0.01268346,  0.04778259,  0.04862589, -0.01601226,\n","       -0.0088468 ,  0.00142456, -0.02146822,  0.040478  ,  0.02297852,\n","       -0.0073338 ,  0.00043194, -0.02969912,  0.00470258,  0.00189666,\n","       -0.00736542, -0.015345  ,  0.03973142, -0.0251552 ,  0.04227995,\n","       -0.04058808,  0.01225681, -0.03755424, -0.03184002, -0.04608554,\n","        0.01537944,  0.04921863,  0.04774326,  0.01021012, -0.0366311 ,\n","        0.03603328, -0.01764224, -0.04283352,  0.04046682,  0.01898314,\n","        0.0223755 , -0.02870865, -0.04147841, -0.01477901,  0.02544143,\n","       -0.02339097, -0.0261385 , -0.04982349, -0.03024095,  0.00847303,\n","       -0.02265978,  0.04554739,  0.03988514, -0.00911113,  0.03405467,\n","        0.02353038,  0.04503698,  0.04402475, -0.02665638, -0.01734836,\n","        0.0280917 , -0.02240281, -0.04318225,  0.02930412, -0.02702424,\n","       -0.01779053, -0.00953505, -0.03084674, -0.0396935 ,  0.04427458,\n","       -0.001948  , -0.00968469, -0.0261657 ,  0.01355014,  0.00100426,\n","       -0.00508515,  0.03416574,  0.02748578, -0.02765082, -0.00286241,\n","        0.03409412, -0.04260147,  0.01293004,  0.0463935 , -0.0089406 ,\n","        0.04510541, -0.0256968 ,  0.04814538,  0.01320381,  0.03054588,\n","        0.03317055,  0.04473395,  0.02033745], dtype=float32)>"]},"metadata":{},"execution_count":23}],"source":["# Continue to check out a single token's embedding\n","sample_embed[0][0]"]},{"cell_type":"markdown","metadata":{"id":"agv2D_DpXJZH"},"source":["This above output is what our the computer sees each word as. When our model looks for patterns in different samples, these values will be updated."]},{"cell_type":"markdown","metadata":{"id":"ImeUhFkSXZzn"},"source":["## Modelling a text dataset\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-inputs-and-outputs-with-shapes-and-models-were-going-to-build.png)\n","*Once you've got your inputs and outputs prepared, it's a matter of figuring out which machine learning model to build in between them to bridge the gap.*\n","\n","Now our words are numbers. Let's build the model! \n","\n","To gain experience we are going to build plenty of models and compare the results.\n","\n","* **Model 0**: Naive Bayes (baseline)\n","* **Model 1**: Feed-forward neural network (dense model)\n","* **Model 2**: LSTM model\n","* **Model 3**: GRU model\n","* **Model 4**: Bidirectional-LSTM model\n","* **Model 5**: 1D Convolutional Neural Network\n","* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n","* **Model 7**: Same as model 6 with 10% of training data\n","\n","Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n","\n","Each experiment will go through the following steps:\n","* Construct the model\n","* Train the model\n","* Make predictions with the model\n","* Track prediction evaluation metrics for later comparison"]},{"cell_type":"markdown","metadata":{"id":"h9qXA3rkZerb"},"source":["## Model 0: Getting a baseline\n","\n","This is the original baseline model that we will use as a benchmark for our other 7 models.\n","\n","We will create this baseline Naive Bayes model using Scikit-Learn Pipeline usng the TF-IDF (term frequency-inverse document frequency) formulat to convert our words to numbers and then model them with the multinomial naive bayes algorithm. \n","\n","[Scikit-Learn documentation for more](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting).\n","\n","[Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This was chosen via referring to the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":616,"status":"ok","timestamp":1658348452769,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"gfc3owohfVZZ","outputId":"5b9f29bd-4cc7-4491-9ace-9287a27a60b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"]},"metadata":{},"execution_count":24}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","\n","# Create tokenization and modelling pipeline\n","model_0 = Pipeline([\n","                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n","                    (\"clf\", MultinomialNB()) # model the text\n","])\n","\n","# Fit the pipeline to hte training data\n","model_0.fit(train_sentences, train_labels)"]},{"cell_type":"markdown","metadata":{"id":"5vHC9e-aqyA6"},"source":["Because we are using a shallow model (Multinomial Naive Bayes), the training is very fast.\n","\n","Let's evaluate it though"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1658348452943,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"mgTYLz4JrFF3","outputId":"fcdceeac-1ce6-45a8-f304-3d25604d51c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["The score of our baseline model is 79.27%\n"]}],"source":["baseline_score = model_0.score(val_sentences, val_labels)\n","print(f\"The score of our baseline model is {baseline_score*100:.2f}%\")"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658348452944,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"oMjrZA6nrKK9","outputId":"065cbdcb-fd59-405d-b60a-1f7e9196ba9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"]},"metadata":{},"execution_count":26}],"source":["# Make some predictions with the baseline model (model_0)\n","baseline_preds = model_0.predict(val_sentences)\n","baseline_preds[:20]"]},{"cell_type":"markdown","metadata":{"id":"HMnNQpPxsQZC"},"source":["## Creating an evaluation function for our model experiments\n","\n","We are gonig to create a helper function that will take an array of predictions and ground truth labels and compute \n","\n","* Accuracy\n","* Precision\n","* Recall\n","* F1-score\n","\n","*Note*: these metrics are great for classification, for regression other metrics are likely better."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"tWXpswvfsy3I","executionInfo":{"status":"ok","timestamp":1658348452944,"user_tz":300,"elapsed":8,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Function to evaluate performance metrics\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","def calculate_results(y_true, y_pred):\n","  \"\"\" \n","    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n","\n","  Args:\n","  -----\n","  y_true = true labels in the form of a 1D array\n","  y_pred = predicted labels in the form of a 1D array\n","\n","  Returns a dictionary of accuracy, precision, recall, f1-score.\n","  \"\"\"\n","\n","  # Calculate model accuracy\n","  model_accuracy = accuracy_score(y_true, y_pred) * 100\n","  # Calculate model precision, recall, and f1 score using weighted average\n","  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n","  model_results = {\"accuracy\": model_accuracy,\n","                   \"precision\": model_precision,\n","                   \"recall\": model_recall,\n","                   \"f1\":model_f1}\n","  return model_results"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658348452945,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"Ge95Mep00HkU","outputId":"b780e5c1-4df2-4f11-b2fb-4dde08969035"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n","       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n","       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n","       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n","       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n","       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n","       0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n","       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n","       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n","       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n","       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n","       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n","       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n","       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n","       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n","       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n","       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n","       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n","       0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n","       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n","       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n","       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n","       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n","       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n","       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0])"]},"metadata":{},"execution_count":28}],"source":["baseline_preds"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1658348453136,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"qO7E1X7YzcsX","outputId":"0120d643-428a-452e-8a66-0d07f3144a8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'accuracy': 79.26509186351706, 'precision': 0.8111390004213173, 'recall': 0.7926509186351706, 'f1': 0.7862189758049549}\n"]}],"source":["# Return baseline results\n","baseline_results = calculate_results(y_true=val_labels,\n","                                     y_pred=baseline_preds)\n","print(baseline_results)"]},{"cell_type":"markdown","metadata":{"id":"oV-T34OIzyX_"},"source":["## Model_1: A Simple Dense Model\n","\n","The first 'deep' model that we are building is going to be a single layer dense model. \n","\n","This model will take the text and labels ad input, tokeniuze the text, create an embedding, find the average of the embedding (Global Average Pooling) and then pass the average through a fully connected (dense) layer with one output unit and a sigmoid activation function.\n","\n","Since we are going to be building numerous TensorFlow models we will import the `create_tensorboard_callback()` function."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"nFNdocbFDqr_","executionInfo":{"status":"ok","timestamp":1658348453137,"user_tz":300,"elapsed":3,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Create TensorBoard callback\n","from helper_functions import create_tensorboard_callback\n","\n","# Create save directory for TB callback\n","SAVE_DIR = \"model_logs\""]},{"cell_type":"code","execution_count":31,"metadata":{"id":"gBjPPuy7Dxyl","executionInfo":{"status":"ok","timestamp":1658348453273,"user_tz":300,"elapsed":139,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Build model_1 with the functional API\n","from tensorflow.keras import layers\n","\n","inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1D strings\n","x = text_vectorizer(inputs) # turn input text into numbers\n","x = embedding(x) # create an embedding of the text converted to numbers\n","x = layers.GlobalAveragePooling1D()(x) # dimensionality reduction to 1D output\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Create the output layer, want binary outputs so using sigmoid\n","model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"]},{"cell_type":"markdown","metadata":{"id":"7XryFHa5ESbQ"},"source":["What does this model do?\n","\n","1) Take a 1D string as input\n","\n","2) Tokenizes using the `text_vecotorizer` function\n","\n","3) Creates an embedding using `embedding`\n","\n","4) Pass the output of hte pooling layer to a desnse layer with sigmoid activation\n","\n","Next we need to compile our model. Because we are dealing with two classes, we will use `binary_crossentropy`. We will also keep with the `Adam()` optimizer."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"TF5HQzXBEw4e","executionInfo":{"status":"ok","timestamp":1658348453274,"user_tz":300,"elapsed":9,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Compile model_1\n","model_1.compile(optimizer=tf.keras.optimizers.Adam(),\n","                loss=\"binary_crossentropy\",\n","                metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1658348453416,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"C2XU0eFwFRlh","outputId":"0df5ba5a-4f02-4e88-87c3-ff6465eac57e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1_dense\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_1 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," global_average_pooling1d (G  (None, 128)              0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,280,129\n","Trainable params: 1,280,129\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Summary of model_1\n","model_1.summary()"]},{"cell_type":"markdown","metadata":{"id":"jqTcQyqzFfN2"},"source":["As we can see, most of the trainable parameters are contained within the embedding layer. Since we have 128 dim embedding, and 10,000 vocab (words) we have 1,280,000 trainable parameters in this layer.\n","\n","Let's try training for 5 epochs (don't forget TensorBoard callback)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12925,"status":"ok","timestamp":1658348466339,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"qdoq8YJ4F3Db","outputId":"46e762de-1114-40ba-e3cf-5cb91c46215f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/simple_dense_model_1/20220720-202054\n","Epoch 1/5\n","215/215 [==============================] - 8s 9ms/step - loss: 0.6094 - accuracy: 0.6916 - val_loss: 0.5357 - val_accuracy: 0.7572\n","Epoch 2/5\n","215/215 [==============================] - 2s 7ms/step - loss: 0.4410 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n","Epoch 3/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8605 - val_loss: 0.4590 - val_accuracy: 0.7900\n","Epoch 4/5\n","215/215 [==============================] - 1s 4ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4641 - val_accuracy: 0.7927\n","Epoch 5/5\n","215/215 [==============================] - 1s 4ms/step - loss: 0.2380 - accuracy: 0.9118 - val_loss: 0.4767 - val_accuracy: 0.7874\n"]}],"source":["# Fit the model\n","model_1_history = model_1.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n","                                                                     experiment_name=\"simple_dense_model_1\")])"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1658348466485,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"mXzhbu0iGI6E","outputId":"902d1dbe-9eac-4dda-c21e-7dee6908c252"},"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 0s 3ms/step - loss: 0.4767 - accuracy: 0.7874\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.47668465971946716, 0.787401556968689]"]},"metadata":{},"execution_count":35}],"source":["# Evaluate the model\n","model_1.evaluate(val_sentences, val_labels)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1658348466485,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"Wl9TIMnUGRXT","outputId":"1c6f3849-c60b-459a-af01-ff1665c0a9a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n"," array([[ 0.00073165,  0.01504801, -0.03425454, ..., -0.04403536,\n","         -0.01042283,  0.01876437],\n","        [ 0.04135862, -0.03945085, -0.03811943, ...,  0.00464735,\n","          0.03163549,  0.029283  ],\n","        [ 0.00684032,  0.05363132, -0.00241554, ..., -0.07082175,\n","         -0.04750704,  0.01448254],\n","        ...,\n","        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n","          0.00308807,  0.02215792],\n","        [ 0.00692343,  0.05942352, -0.01975194, ..., -0.0619906 ,\n","         -0.01018393,  0.03510419],\n","        [-0.03723461,  0.06267187, -0.07451147, ..., -0.02367218,\n","         -0.0864333 ,  0.01742156]], dtype=float32)>]"]},"metadata":{},"execution_count":36}],"source":["embedding.weights"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1658348466617,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"xOIV238lOxcp","outputId":"e7d809b4-e1bb-4281-eb10-531a50b77aa9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 128)\n"]}],"source":["embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n","print(embed_weights.shape)"]},{"cell_type":"markdown","metadata":{"id":"fFVeplcPO5rK"},"source":["Let's leverage the TensorBoard callback to visualize out model's logs"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"62SfHxl9Pjd3","executionInfo":{"status":"ok","timestamp":1658348466618,"user_tz":300,"elapsed":4,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n","# # Upload TensorBoard dev records\n","# !tensorboard dev upload --logdir ./model_logs \\\n","#   --name \"First deep model on text data\" \\\n","#   --description \"Trying a dense model with an embedding layer\" \\\n","#   --one_shot # exits the uploader when upload has finished"]},{"cell_type":"markdown","metadata":{"id":"zfmr8kxBPmXA"},"source":["![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tensorboard-dense-model-training-curves.png)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162,"status":"ok","timestamp":1658348466777,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"buQgMHAUP2Mp","outputId":"9504b1ed-795f-4979-a033-2444fe4e1cb1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.40488204],\n","       [0.7443312 ],\n","       [0.997895  ],\n","       [0.10889999],\n","       [0.11143532],\n","       [0.93556094],\n","       [0.9134595 ],\n","       [0.9925345 ],\n","       [0.97156817],\n","       [0.26570338]], dtype=float32)"]},"metadata":{},"execution_count":39}],"source":["# Make predictions\n","model_1_pred_probs = model_1.predict(val_sentences)\n","model_1_pred_probs[:10]"]},{"cell_type":"markdown","metadata":{"id":"MKwkoTniQf3e"},"source":["Since our output layer uses a sigmoid activation function, we get these probabilities returned as our predictions.\n","\n","Essentially we just round them using `tf.round()` to get 0 or 1. But we also get insights into how certain the network is.\n","\n","*Note:* In practice, the output threshold of a sigmoid prediction probability doesn't necessarily have to 0.5. For example, through testing, you may find that a cut off of 0.25 is better for your chosen evaluation metrics. A common example of this threshold cutoff is the [precision-recall tradeoff](https://www.machinelearningaptitude.com/topics/machine-learning/what-is-precision-recall-tradeoff/#:~:text=precision%2Drecall%20tradeoff%20occur%20due,the%20threshold%20of%20the%20classifier.&text=When%20threshold%20is%20decreased%20to,but%20precision%20decreases%20to%200.4.)."]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1658348466951,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"PHqV-Nu9Q9Oi","outputId":"4b8178d5-6236-4397-ff03-02163d46fd00"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(762, 1), dtype=float32, numpy=\n","array([[0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [1.],\n","       [1.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.]], dtype=float32)>"]},"metadata":{},"execution_count":40}],"source":["# Turn prediciton probabilities into tensor of floats\n","model_1_preds = tf.round(model_1_pred_probs)\n","model_1_preds"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1658348466951,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"4aQt-tf2RRUd","outputId":"f35889e8-b30c-4103-954b-ceb4b0ebf8c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(762,), dtype=float32, numpy=\n","array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n","       0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n","       1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n","       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n","       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n","       0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n","       0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n","       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n","       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n","       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n","       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n","       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n","       0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n","       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n","       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n","       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n","       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n","       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n","       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n","       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n","       0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n","       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n","       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n","       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n","       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n","       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n","       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n","       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n","       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n","       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n","       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n","       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n","       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n","       0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n","       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n","       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n","       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n","       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n","       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n","       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n","       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n","      dtype=float32)>"]},"metadata":{},"execution_count":41}],"source":["# Turn prediciton probabilities into single-dimension tensor of floats\n","\n","model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n","model_1_preds"]},{"cell_type":"markdown","metadata":{"id":"Jax9MqPZRVAu"},"source":["This now outputs our model's predictiosn in the form of classes which can be used in our `calculate results()` function to compare them to the ground truth validation labels"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658348466952,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"BSNCsWZaR70A","outputId":"853049c1-e065-4663-9f1c-65b5b20cb79f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 78.74015748031496,\n"," 'f1': 0.7846966492209201,\n"," 'precision': 0.7914920592553047,\n"," 'recall': 0.7874015748031497}"]},"metadata":{},"execution_count":42}],"source":["# Calculate model_1 results\n","model_1_results = calculate_results(y_true=val_labels,\n","                                    y_pred=model_1_preds)\n","model_1_results"]},{"cell_type":"markdown","metadata":{"id":"i25kPYJJSZcV"},"source":["Let's compare to our model_0"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658348466952,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"-Zr1s6aZTKXm","outputId":"94624f1e-f029-45e9-d28e-ef4dbe257c52"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False, False, False, False])"]},"metadata":{},"execution_count":43}],"source":["# Is our simple model better than the baseline?\n","import numpy as np\n","\n","np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"]},{"cell_type":"markdown","metadata":{"id":"-wP6io4rTV6B"},"source":["Looks like it is not better.\n","\n","We will do this often, so let's make a function of it."]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658348466952,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"mBwmwUGDTaRQ","outputId":"3783bd4a-84f9-4190-e9cc-d40e8eb8f5d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n","Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n","Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n","Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"]}],"source":["# Create helper function to compare baseline results to new model results\n","def compare_baseline_to_new_results(baseline_results, new_model_results):\n","  for key, value in baseline_results.items():\n","    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n","\n","compare_baseline_to_new_results(baseline_results=baseline_results, new_model_results=model_1_results)"]},{"cell_type":"markdown","metadata":{"id":"BsZ_Q4N9T8fG"},"source":["## Visualizing learning embeddings\n","\n","`model_1` contains an embedding layer (`embedding`) which learned a way of representing words as feature vectors by passing ovewr the training data.\n","\n","To help us better understand the embedding layer let's try to visualize it a bit."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658348466952,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"krL7gi1lU0Wu","outputId":"85151d9d-1480-4481-a3e2-442659c3d02a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"]},"metadata":{},"execution_count":45}],"source":["# Remind ourselves of the words in our vocab\n","# Get the vocab from the text vectorization layer\n","words_in_vocab = text_vectorizer.get_vocabulary()\n","len(words_in_vocab), words_in_vocab[:10]"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1658348467090,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"TmZeSpq2VA4q","outputId":"37e284ab-270e-4737-bdfe-60b8980bab4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1_dense\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_1 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," global_average_pooling1d (G  (None, 128)              0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,280,129\n","Trainable params: 1,280,129\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model_1.summary()"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658348467090,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"Tb1AiTCzWPGP","outputId":"fda9566c-4703-4429-dcce-2971ec3c1cae"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 128)\n"]}],"source":["# Get the weight of the embedding layer\n","# Note: these are the numerical patterns between the text adn the training dataset the model has learned\n","\n","embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n","print(embed_weights.shape)"]},{"cell_type":"markdown","metadata":{"id":"GdkzXxDtWrxh"},"source":["Now we've got these two objects, we can use the [Embedding Projector tool](http://projector.tensorflow.org/_) to visualize our embedding. \n","\n","To use the Embedding Projector tool, we need two files:\n","* The embedding vectors (same as embedding weights).\n","* The meta data of the embedding vectors (the words they represent - our vocabulary).\n","\n","Right now, we've got of these files as Python objects. To download them to file, we're going to [use the code example available on the TensorFlow word embeddings tutorial page](https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk).\n"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":777,"status":"ok","timestamp":1658348467866,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"u6FJusJUXJLO","outputId":"8dad6c3e-45a2-4521-8912-aecb1a731d86"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5749643c-f0f2-44b5-8345-418b3b6fe3d5\", \"embedding_vectors.tsv\", 15389792)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_00d7d02e-3c92-45aa-a4ca-7b917a9694e0\", \"embedding_metadata.tsv\", 80388)"]},"metadata":{}}],"source":["# Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n","import io\n","\n","# Create output writers\n","out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n","out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n","\n","# Write embedding vectors and words to file\n","for num, word in enumerate(words_in_vocab):\n","  if num == 0: \n","     continue # skip padding token\n","  vec = embed_weights[num]\n","  out_m.write(word + \"\\n\") # write words to file\n","  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n","out_v.close()\n","out_m.close()\n","\n","# Download files locally to upload to Embedding Projector\n","try:\n","  from google.colab import files\n","except ImportError:\n","  pass\n","else:\n","  files.download(\"embedding_vectors.tsv\")\n","  files.download(\"embedding_metadata.tsv\")"]},{"cell_type":"markdown","metadata":{"id":"vRNL9ZSaXPZF"},"source":["Once you've downloaded the embedding vectors and metadata, you can visualize them using Embedding Vector tool:\n","1. Go to  http://projector.tensorflow.org/\n","2. Click on \"Load data\"\n","3. Upload the two files you downloaded (`embedding_vectors.tsv` and `embedding_metadata.tsv`)\n","\n","Note: words with similar meanings shouuld be close together."]},{"cell_type":"markdown","metadata":{"id":"iRlfr643Xq9t"},"source":["## Recurrent Neural Networks (RNNs)\n","\n","For the next models we will be using a type of NN known as **Recurrent Neural Network**.\n","\n","RNNs use information from the past to help with the future.  It takes an input and computes an output based on all previous results, though the weights of earlier previous results decreases rapidly.\n","\n","This concept is great for short passages of natural text as it provides some context.\n","\n","For a simple example, take two sentences:\n","1. Massive earthquake last week, no?\n","2. No massive earthquake last week.\n","\n","Both contain exactly the same words but have different meaning. The order of the words determines the meaning (one could argue punctuation marks also dictate the meaning but for simplicity sake, let's stay focused on the words).\n","\n","Recurrent neural networks can be used for a number of sequence-based problems:\n","* **One to one:** one input, one output, such as image classification.\n","* **One to many:** one input, many outputs, such as image captioning (image input, a sequence of text as caption output).\n","* **Many to one:** many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).\n","* **Many to many:** many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).\n","\n","Most applications of RNNs are of the following variants: \n","* Long short-term memory cells (LSTMs).\n","* Gated recurrent units (GRUs).\n","* Bidirectional RNN's (passes forward and backward along a sequence, left to right and right to left).\n","\n","For a deeper understanding of what's happening check out the following resources:\n","\n","* [MIT Deep Learning Lecture on Recurrent Neural Networks](https://youtu.be/SEnXr6v2ifU) - explains the background of recurrent neural networks and introduces LSTMs.\n","* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy - demonstrates the power of RNN's with examples generating various sequences.\n","* [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block.\n"]},{"cell_type":"markdown","metadata":{"id":"TIGHV3vgayBX"},"source":["## Model_2: LSTM\n","\n","This will be an LSTM-powered LSTM leveraging `tensorflow.keras.layers.LSTM()`.\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-RNN-architecture-coloured-block-edition.png)\n","*Coloured block example of the structure of an recurrent neural network.*\n","\n","The structure of model_2 will be very similar to that of model_1. (Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probabilities))\n","\n","To ensure we're not getting reusing trained embeddings (this would involve data leakage between models, leading to an uneven comparison later on), we'll create another embedding layer (`model_2_embedding`) for our model. The `text_vectorizer` layer can be reused since it doesn't get updated during training."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471,"status":"ok","timestamp":1658348468330,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"OZNVoJ5rdrIY","outputId":"c9aafc55-c733-4312-9674-1605920bc6b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 15, 128)\n","(None, 64)\n"]}],"source":["# Set random seed and create embedding layer\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","\n","model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_2\")\n","\n","# Create the LSTM model\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_2_embedding(x)\n","print(x.shape)\n","# x = layers.LSTM(64, return_sequences=True)(x) return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n","x = layers.LSTM(64)(x) # return vector for whole sequence\n","print(x.shape)\n","# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"b-l807pffs6W","executionInfo":{"status":"ok","timestamp":1658348468330,"user_tz":300,"elapsed":19,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Compile model\n","model_2.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1658348468331,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"d3ipP0pSgCX4","outputId":"914d38b9-d14f-495b-fb31-63ce44738508"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2_LSTM\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_2 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," lstm (LSTM)                 (None, 64)                49408     \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1,329,473\n","Trainable params: 1,329,473\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model_2.summary()"]},{"cell_type":"markdown","metadata":{"id":"vTsp5DI3gFa0"},"source":["model_2 has more trainable parameters than model_1.\n","\n","Let's fit with out TB callback"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11495,"status":"ok","timestamp":1658348479821,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"JVnr2569gOb-","outputId":"9e2182ea-8077-45bd-bea3-7588ac8377fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/LSTM/20220720-202109\n","Epoch 1/5\n","215/215 [==============================] - 6s 8ms/step - loss: 0.5100 - accuracy: 0.7416 - val_loss: 0.4566 - val_accuracy: 0.7822\n","Epoch 2/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.3176 - accuracy: 0.8717 - val_loss: 0.5138 - val_accuracy: 0.7756\n","Epoch 3/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.2201 - accuracy: 0.9152 - val_loss: 0.5858 - val_accuracy: 0.7677\n","Epoch 4/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.6041 - val_accuracy: 0.7743\n","Epoch 5/5\n","215/215 [==============================] - 1s 7ms/step - loss: 0.1076 - accuracy: 0.9594 - val_loss: 0.8746 - val_accuracy: 0.7507\n"]}],"source":["# Fit model\n","model_2_history = model_2.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n","                                                                     \"LSTM\")])"]},{"cell_type":"markdown","metadata":{"id":"YvunvFRJga-c"},"source":["Let's make some predictions now. We have the same sigmoid activation layer as with model_1 so we need to `tf.round()` and `tf.squeeze` again."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1160,"status":"ok","timestamp":1658348480979,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"ufxAxtGzg-x2","outputId":"5b5cc006-143e-4b66-89c6-ebe37042c1f7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((762, 1), array([[0.007126  ],\n","        [0.7873678 ],\n","        [0.9996376 ],\n","        [0.05679173],\n","        [0.00258219],\n","        [0.9996238 ],\n","        [0.9217021 ],\n","        [0.9997993 ],\n","        [0.9994954 ],\n","        [0.6645738 ]], dtype=float32))"]},"metadata":{},"execution_count":53}],"source":["# Make predictions on the validation dataset\n","model_2_pred_probs = model_2.predict(val_sentences)\n","model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658348480979,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"Ks0xzGqOhCDk","outputId":"4451a1fb-4984-4e1c-9cb9-5512d12a451c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"]},"metadata":{},"execution_count":54}],"source":["# Round out predictions and reduce to 1-dimensional array\n","model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n","model_2_preds[:10]"]},{"cell_type":"markdown","metadata":{"id":"-3otj6V3hCOT"},"source":["Let's calcualte our results from model_2 and compare it to the baseline"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1658348480979,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"_XY2y2eohdsk","outputId":"5bd9348b-c2c6-41a1-80ad-e0a3cfd2e30c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 75.06561679790026,\n"," 'f1': 0.7489268622514025,\n"," 'precision': 0.7510077975908164,\n"," 'recall': 0.7506561679790026}"]},"metadata":{},"execution_count":55}],"source":["# Calcualte results of the LSTM model\n","model_2_results = calculate_results(y_true=val_labels,\n","                                    y_pred=model_2_preds)\n","model_2_results"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1658348481101,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"l9mSBFLUhmjk","outputId":"3dab8bc8-3d2b-46bb-cb5b-afdb6b127f94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n","Baseline precision: 0.81, New precision: 0.75, Difference: -0.06\n","Baseline recall: 0.79, New recall: 0.75, Difference: -0.04\n","Baseline f1: 0.79, New f1: 0.75, Difference: -0.04\n"]}],"source":["# Compare to baseline\n","compare_baseline_to_new_results(baseline_results, model_2_results)"]},{"cell_type":"markdown","metadata":{"id":"jWkLp9mEhr99"},"source":["## Model_3 GRU\n","\n","Alternatively to LSTM there is the **GRU** or *Gated Recurrent Unit*.\n","\n","A GRU cell has similar features to an LSTM but with less parameters.\n","\n","We will use `tf.keras.layers.GRU()` to create a GRU layer wiin tensorflow.\n","\n","A few resources to dig into GRUs further:\n","* [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) Wikipedia page\n","* [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be) by Simeon Kostadinov"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"nAWYs2i4oHx5","executionInfo":{"status":"ok","timestamp":1658348481566,"user_tz":300,"elapsed":469,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Set random seed and create new embedding layer\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","\n","model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_3\")\n","\n","# Build an RNN using the GRU cell\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_3_embedding(x)\n","# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\n","x = layers.GRU(64)(x) \n","# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"kgpin4egpR9l","executionInfo":{"status":"ok","timestamp":1658348481566,"user_tz":300,"elapsed":6,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Compile GRU model\n","model_3.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658348481567,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"t2LKknGCqQtl","outputId":"ed131b53-dc38-4112-e15d-a100e7403e52"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3_GRU\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_3 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," gru (GRU)                   (None, 64)                37248     \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1,317,313\n","Trainable params: 1,317,313\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model_3.summary()"]},{"cell_type":"markdown","metadata":{"id":"4rg4PYumqWqC"},"source":["Notice the difference in number of trainable parameters between `model_2` (LSTM) and `model_3` (GRU). The difference comes from the LSTM cell having more trainable parameters than the GRU cell."]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14280,"status":"ok","timestamp":1658348495844,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"Rl_XxF4MqScL","outputId":"34a71d9d-434d-47b0-931a-fe29c67d8b6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/GRU/20220720-202122\n","Epoch 1/5\n","215/215 [==============================] - 6s 14ms/step - loss: 0.5242 - accuracy: 0.7314 - val_loss: 0.4553 - val_accuracy: 0.7769\n","Epoch 2/5\n","215/215 [==============================] - 2s 10ms/step - loss: 0.3195 - accuracy: 0.8694 - val_loss: 0.4937 - val_accuracy: 0.7808\n","Epoch 3/5\n","215/215 [==============================] - 2s 10ms/step - loss: 0.2197 - accuracy: 0.9181 - val_loss: 0.5607 - val_accuracy: 0.7743\n","Epoch 4/5\n","215/215 [==============================] - 2s 9ms/step - loss: 0.1599 - accuracy: 0.9441 - val_loss: 0.6220 - val_accuracy: 0.7782\n","Epoch 5/5\n","215/215 [==============================] - 2s 10ms/step - loss: 0.1221 - accuracy: 0.9584 - val_loss: 0.6205 - val_accuracy: 0.7677\n"]}],"source":["# fit model_3\n","model_3_history = model_3.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR,\"GRU\")])"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1658348496281,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"mGXRpV79qkUV","outputId":"d9c0d7ae-5700-4296-f81c-2f737f3de42d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((762, 1), array([[0.33325258],\n","        [0.87741184],\n","        [0.9980252 ],\n","        [0.11561756],\n","        [0.01235959],\n","        [0.9925639 ],\n","        [0.6214262 ],\n","        [0.99813336],\n","        [0.9982377 ],\n","        [0.5018107 ]], dtype=float32))"]},"metadata":{},"execution_count":61}],"source":["# Predictions on the val_data\n","model_3_pred_probs = model_3.predict(val_sentences)\n","model_3_pred_probs.shape, model_3_pred_probs[:10]"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658348496281,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"MK40bq9ntFg1","outputId":"961e094e-f37c-4b12-ec93-4d7ce1020053"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"]},"metadata":{},"execution_count":62}],"source":["# Convert to prediction classes\n","model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n","model_3_preds[:10]"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130,"status":"ok","timestamp":1658348496408,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"7ZkUMvmAtIhE","outputId":"aef67137-f426-441d-d3ee-bad2e0931f85"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 76.77165354330708,\n"," 'f1': 0.7667932666650168,\n"," 'precision': 0.7675450859410361,\n"," 'recall': 0.7677165354330708}"]},"metadata":{},"execution_count":63}],"source":["# Calcuate model_3 results\n","model_3_results = calculate_results(y_true=val_labels, \n","                                    y_pred=model_3_preds)\n","model_3_results"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658348496409,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"byTEH3fKtKLu","outputId":"ca9b3a64-51f0-49df-9c18-5d6a7b88f4eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\n","Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n","Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n","Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"]}],"source":["# Compare to baseline\n","compare_baseline_to_new_results(baseline_results, model_3_results)"]},{"cell_type":"markdown","metadata":{"id":"BdbSKIIHtMN4"},"source":["## Model_4: Bidirectional RNN model\n","\n","`model_4` will be a **bidirectional RNN**. A standard RNN will process a sequence from left to right, where a bidirectional RNN will process the sequence from left to right and then again from the right to the left.\n","\n","Bidirectional RNNs are a tried and true way of improving perfomance in many applications. The downside being that the training time is increased and the number of model parameters is doubled. \n","\n","We will implement this bidirectional layer with the `tf.keras.layers.Bidirectional` class. We can use this to wrap out existing RNNs."]},{"cell_type":"code","execution_count":65,"metadata":{"id":"4Gp6I51g2QYr","executionInfo":{"status":"ok","timestamp":1658348497513,"user_tz":300,"elapsed":1112,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Set random seed and create embedding layer\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","model_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_4\")\n","\n","# Build a bidirectional RNN in TensorFlow\n","input = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_4_embedding(x)\n","x = layers.Bidirectional(layers.LSTM(64))(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_bidirectional\")"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"7j2SFBSK26dk","executionInfo":{"status":"ok","timestamp":1658348497513,"user_tz":300,"elapsed":8,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"outputs":[],"source":["# Compile\n","model_4.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1658348497666,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"v0bfnjTJ4YyW","outputId":"258b89c3-d335-42ab-fb73-568e5d2f9ff6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4_bidirectional\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_4 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 128)              98816     \n"," l)                                                              \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,378,945\n","Trainable params: 1,378,945\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Summary\n","model_4.summary()"]},{"cell_type":"markdown","metadata":{"id":"CzLuiO-44b-w"},"source":["From the summary output we can see that the bidirectionality that we added to the model has significantly increased the number of trainable parameters."]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17362,"status":"ok","timestamp":1658348515026,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"uznb7IJa45oh","outputId":"ede292a7-75c9-496e-b969-0c043e2f54c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/bidirectional_RNN/20220720-202138\n","Epoch 1/5\n","215/215 [==============================] - 7s 16ms/step - loss: 0.5093 - accuracy: 0.7481 - val_loss: 0.4606 - val_accuracy: 0.7795\n","Epoch 2/5\n","215/215 [==============================] - 3s 12ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.5144 - val_accuracy: 0.7690\n","Epoch 3/5\n","215/215 [==============================] - 3s 13ms/step - loss: 0.2150 - accuracy: 0.9178 - val_loss: 0.5626 - val_accuracy: 0.7677\n","Epoch 4/5\n","215/215 [==============================] - 3s 13ms/step - loss: 0.1523 - accuracy: 0.9469 - val_loss: 0.6365 - val_accuracy: 0.7769\n","Epoch 5/5\n","215/215 [==============================] - 2s 8ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.6509 - val_accuracy: 0.7664\n"]}],"source":["# Fit the model\n","model_4_history = model_4.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":823,"status":"ok","timestamp":1658348515846,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"dsm-4iFG5MJ7","outputId":"b3c12844-32c4-4488-8fa4-0170625ab791"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.04000054],\n","       [0.8279291 ],\n","       [0.99842227],\n","       [0.13531098],\n","       [0.00311338],\n","       [0.99220747],\n","       [0.95528346],\n","       [0.99945647],\n","       [0.99898285],\n","       [0.2814168 ]], dtype=float32)"]},"metadata":{},"execution_count":69}],"source":["# Making some predictions\n","model_4_pred_probs = model_4.predict(val_sentences)\n","model_4_pred_probs[:10]"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658348515846,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"qDqyPIFW6Ft_","outputId":"48adc24e-b713-4da3-a222-06761f53929f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":70}],"source":["# Convert prediction probabilities to class labels\n","model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n","model_4_preds[:10]"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658348515846,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"2IaJrCbO6PdF","outputId":"82333ba3-4bf3-404d-dea5-20788404bdb6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 76.64041994750657,\n"," 'f1': 0.7651213533864446,\n"," 'precision': 0.7665895370389821,\n"," 'recall': 0.7664041994750657}"]},"metadata":{},"execution_count":71}],"source":["# Calculate results for model_4\n","model_4_results = calculate_results(val_labels, model_4_preds)\n","model_4_results"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658348515846,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"ePotG_Kx6Yeq","outputId":"1954f375-d28b-4a33-8c88-2dab13cd3c3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 76.64, Difference: -2.62\n","Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n","Baseline recall: 0.79, New recall: 0.77, Difference: -0.03\n","Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"]}],"source":["compare_baseline_to_new_results(baseline_results, model_4_results)"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658348515847,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"sp_d5tvZ9xTK","outputId":"400a9978-9185-4d6d-a1ee-c060f0bb768f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\n","Baseline precision: 0.81, New precision: 0.77, Difference: -0.04\n","Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n","Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"]}],"source":["# Running compare against all as a reminder\n","compare_baseline_to_new_results(baseline_results, model_3_results)"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658348515847,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"7olrU69g92vk","outputId":"fcf87bc2-1e74-4e4e-f40b-4c3dada4448a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n","Baseline precision: 0.81, New precision: 0.75, Difference: -0.06\n","Baseline recall: 0.79, New recall: 0.75, Difference: -0.04\n","Baseline f1: 0.79, New f1: 0.75, Difference: -0.04\n"]}],"source":["compare_baseline_to_new_results(baseline_results, model_2_results)"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658348515847,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"PBJ7rpFL93jb","outputId":"0b199524-3f4a-432c-f0de-38ad65feb462"},"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n","Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n","Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n","Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"]}],"source":["compare_baseline_to_new_results(baseline_results, model_1_results)"]},{"cell_type":"markdown","metadata":{"id":"Qevm163o94iA"},"source":["Weird. It seems like none of these are better really than the simple baseline. What's going on?"]},{"cell_type":"markdown","metadata":{"id":"yK2P2cjr-OU-"},"source":["## Model_5: Conv1D\n","\n","Beforfe we build a full 1-D CNN model we are going to build a 1-D convolutional layer (aka **temporal convolution**) in action.\n","\n","This involves \n","\n","(1) embedding \n","\n","(2) Conv1D() \n","\n","(3) GlobalMaxPool1D\n","\n","The intuition here is explained succinctly in the paper [*Understanding Convolutional Neural Networks for Text Classification*](https://www.aclweb.org/anthology/W18-5408.pdf), where they state that CNNs classify text through the following steps:\n","1. 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams (an ngram is a collection of n-words, for example, an ngram of 5 might result in \"hello, my name is Daniel\").\n","2. Max-pooling over time extracts the relevant ngrams for making a decision.\n","3. The rest of the network classifies the text based on this information.\n"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5758,"status":"ok","timestamp":1658348521602,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"},"user_tz":300},"id":"NKOx62N1-lsv","outputId":"5a056ad5-fbb8-4b1a-9253-def65d948da4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"]},"metadata":{},"execution_count":76}],"source":["# Test out the embedding, 1D conv and maxpooling\n","embedding_test = embedding(text_vectorizer([\"this is a test sentence\"]))\n","conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over the target sequence 5 words (kernels?) at a time\n","conv_1d_output = conv_1d(embedding_test)\n","max_pool = layers.GlobalMaxPool1D()\n","max_pool_output = max_pool(conv_1d_output)\n","embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"]},{"cell_type":"markdown","source":["Remember we set our input length to 15..."],"metadata":{"id":"cY956EvbARvx"}},{"cell_type":"code","source":["# See each layers' outputs\n","embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbvTOxzMfDDd","executionInfo":{"status":"ok","timestamp":1658348521743,"user_tz":300,"elapsed":149,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"2b0b7925-6bc8-4295-ad41-404dd78380ab"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n"," array([[[ 0.02534915, -0.03109057,  0.00285616, ..., -0.00783159,\n","          -0.02685575, -0.04434131],\n","         [-0.06586257,  0.09451495, -0.01477601, ..., -0.00657781,\n","          -0.04238791,  0.07777896],\n","         [-0.04803654, -0.00709756, -0.02330893, ..., -0.01807331,\n","           0.02351035,  0.02676385],\n","         ...,\n","         [ 0.00073165,  0.01504801, -0.03425454, ..., -0.04403536,\n","          -0.01042283,  0.01876437],\n","         [ 0.00073165,  0.01504801, -0.03425454, ..., -0.04403536,\n","          -0.01042283,  0.01876437],\n","         [ 0.00073165,  0.01504801, -0.03425454, ..., -0.04403536,\n","          -0.01042283,  0.01876437]]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n"," array([[[0.08324981, 0.00648718, 0.        , 0.03983573, 0.        ,\n","          0.01144417, 0.00416251, 0.02288391, 0.        , 0.00900979,\n","          0.        , 0.        , 0.0340177 , 0.06408274, 0.08103721,\n","          0.00409015, 0.01579619, 0.        , 0.07930177, 0.        ,\n","          0.        , 0.        , 0.14525086, 0.        , 0.        ,\n","          0.        , 0.03682076, 0.06534284, 0.        , 0.        ,\n","          0.05094625, 0.        ],\n","         [0.        , 0.05387189, 0.        , 0.11491333, 0.        ,\n","          0.        , 0.1623708 , 0.        , 0.        , 0.00171252,\n","          0.14336711, 0.        , 0.        , 0.        , 0.        ,\n","          0.01197934, 0.        , 0.        , 0.1355137 , 0.0040106 ,\n","          0.10309822, 0.09445543, 0.08390295, 0.        , 0.04213032,\n","          0.04487596, 0.0656046 , 0.        , 0.02272682, 0.        ,\n","          0.        , 0.        ],\n","         [0.03683224, 0.04895763, 0.        , 0.15324755, 0.        ,\n","          0.        , 0.        , 0.        , 0.        , 0.        ,\n","          0.        , 0.04650313, 0.00496455, 0.07349402, 0.01608641,\n","          0.        , 0.02779122, 0.        , 0.08080561, 0.01403173,\n","          0.        , 0.03768813, 0.10382783, 0.        , 0.0336166 ,\n","          0.        , 0.02577608, 0.00140356, 0.        , 0.        ,\n","          0.03211498, 0.        ],\n","         [0.00887822, 0.10450974, 0.        , 0.06974535, 0.02328685,\n","          0.        , 0.0405221 , 0.        , 0.        , 0.02733762,\n","          0.08674343, 0.        , 0.        , 0.0612985 , 0.02007267,\n","          0.        , 0.        , 0.        , 0.03364266, 0.        ,\n","          0.04525331, 0.05219702, 0.06375708, 0.        , 0.        ,\n","          0.00774408, 0.00273469, 0.        , 0.        , 0.00499633,\n","          0.        , 0.        ],\n","         [0.        , 0.02369072, 0.        , 0.05827616, 0.0529764 ,\n","          0.        , 0.        , 0.        , 0.        , 0.        ,\n","          0.01719717, 0.02936819, 0.00466102, 0.06879884, 0.01944805,\n","          0.01585532, 0.01294547, 0.        , 0.06866534, 0.        ,\n","          0.00623767, 0.03514051, 0.02407537, 0.        , 0.05979814,\n","          0.        , 0.01170143, 0.        , 0.        , 0.        ,\n","          0.04444931, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.05054972, 0.06105435,\n","          0.        , 0.00997431, 0.01403008, 0.        , 0.01680727,\n","          0.03148505, 0.03889384, 0.        , 0.07710679, 0.00590968,\n","          0.        , 0.00263034, 0.        , 0.08935828, 0.        ,\n","          0.        , 0.0533115 , 0.05227956, 0.        , 0.06658382,\n","          0.01881707, 0.02448698, 0.        , 0.        , 0.        ,\n","          0.02008462, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.05054971, 0.06105435,\n","          0.        , 0.0099743 , 0.0140301 , 0.        , 0.01680727,\n","          0.03148504, 0.03889386, 0.        , 0.07710678, 0.00590967,\n","          0.        , 0.00263035, 0.        , 0.08935829, 0.        ,\n","          0.        , 0.0533115 , 0.05227955, 0.        , 0.06658383,\n","          0.01881707, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.02008461, 0.        ],\n","         [0.03544864, 0.        , 0.        , 0.05054972, 0.06105436,\n","          0.        , 0.00997429, 0.0140301 , 0.        , 0.01680727,\n","          0.03148504, 0.03889386, 0.        , 0.07710678, 0.00590968,\n","          0.        , 0.00263035, 0.        , 0.08935829, 0.        ,\n","          0.        , 0.0533115 , 0.05227956, 0.        , 0.06658381,\n","          0.01881707, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.02008462, 0.        ],\n","         [0.03544864, 0.        , 0.        , 0.05054971, 0.06105435,\n","          0.        , 0.00997429, 0.0140301 , 0.        , 0.01680728,\n","          0.03148504, 0.03889385, 0.        , 0.07710677, 0.00590967,\n","          0.        , 0.00263036, 0.        , 0.08935829, 0.        ,\n","          0.        , 0.0533115 , 0.05227955, 0.        , 0.06658383,\n","          0.01881707, 0.02448698, 0.        , 0.        , 0.        ,\n","          0.02008461, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.05054972, 0.06105434,\n","          0.        , 0.00997431, 0.0140301 , 0.        , 0.01680727,\n","          0.03148504, 0.03889385, 0.        , 0.07710678, 0.00590968,\n","          0.        , 0.00263035, 0.        , 0.08935829, 0.        ,\n","          0.        , 0.05331152, 0.05227955, 0.        , 0.06658383,\n","          0.01881707, 0.02448699, 0.        , 0.        , 0.        ,\n","          0.02008461, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.05054972, 0.06105435,\n","          0.        , 0.0099743 , 0.01403009, 0.        , 0.01680728,\n","          0.03148504, 0.03889384, 0.        , 0.07710678, 0.00590968,\n","          0.        , 0.00263035, 0.        , 0.08935827, 0.        ,\n","          0.        , 0.05331151, 0.05227956, 0.        , 0.06658383,\n","          0.01881706, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.02008461, 0.        ]]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n"," array([[0.08324981, 0.10450974, 0.        , 0.15324755, 0.06105436,\n","         0.01144417, 0.1623708 , 0.02288391, 0.        , 0.02733762,\n","         0.14336711, 0.04650313, 0.0340177 , 0.07710679, 0.08103721,\n","         0.01585532, 0.02779122, 0.        , 0.1355137 , 0.01403173,\n","         0.10309822, 0.09445543, 0.14525086, 0.        , 0.06658383,\n","         0.04487596, 0.0656046 , 0.06534284, 0.02272682, 0.00499633,\n","         0.05094625, 0.        ]], dtype=float32)>)"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["Let's create a model now that we have seen some individual outputs"],"metadata":{"id":"9dyELy9AfMz3"}},{"cell_type":"code","source":["# Set random seed and create new embedding\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_5\")\n","\n","# Create 1D conv. layer\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_5_embedding(x)\n","x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\n","x = layers.GlobalMaxPool1D()(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_conv1D\")\n","\n","# Compile\n","model_5.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","# Summary\n","model_5.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sxp1vWHx2W6v","executionInfo":{"status":"ok","timestamp":1658348521872,"user_tz":300,"elapsed":134,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"9e7f923c-10ff-4fe0-b124-dc2bb9b1c596"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_5_conv1D\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_5 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 11, 32)            20512     \n","                                                                 \n"," global_max_pooling1d_1 (Glo  (None, 32)               0         \n"," balMaxPooling1D)                                                \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 1,300,545\n","Trainable params: 1,300,545\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Fit the model\n","model_5_history = model_5.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n","                                                                     \"Conv1D\")])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGgRBwTD20tL","executionInfo":{"status":"ok","timestamp":1658348528991,"user_tz":300,"elapsed":7120,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"ec2c069f-e27e-4924-e2ba-7182c0b3d618"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/Conv1D/20220720-202202\n","Epoch 1/5\n","215/215 [==============================] - 3s 6ms/step - loss: 0.5652 - accuracy: 0.7141 - val_loss: 0.4733 - val_accuracy: 0.7795\n","Epoch 2/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8615 - val_loss: 0.4758 - val_accuracy: 0.7730\n","Epoch 3/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.2070 - accuracy: 0.9234 - val_loss: 0.5457 - val_accuracy: 0.7730\n","Epoch 4/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.1314 - accuracy: 0.9578 - val_loss: 0.6163 - val_accuracy: 0.7730\n","Epoch 5/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.0933 - accuracy: 0.9691 - val_loss: 0.6779 - val_accuracy: 0.7782\n"]}]},{"cell_type":"code","source":["# Make predictions model_5\n","model_5_pred_probs = model_5.predict(val_sentences)\n","model_5_pred_probs[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYaeXeLD3DTY","executionInfo":{"status":"ok","timestamp":1658348529264,"user_tz":300,"elapsed":276,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"72db4d87-9849-4b30-b76c-44bc707c267b"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.22534487],\n","       [0.7534112 ],\n","       [0.9995602 ],\n","       [0.0556279 ],\n","       [0.01449847],\n","       [0.9858518 ],\n","       [0.98418933],\n","       [0.99758804],\n","       [0.99862623],\n","       [0.26914388]], dtype=float32)"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["# Convert model_5 predictions to labels\n","model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n","model_5_preds[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JocRVc473Sp9","executionInfo":{"status":"ok","timestamp":1658348529265,"user_tz":300,"elapsed":16,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"e5c9db76-78f7-4038-cd08-981ce8d292ec"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["# Calculate model_5 evaluation metrics\n","model_5_results = calculate_results(y_true=val_labels,\n","                                    y_pred=model_5_preds)\n","model_5_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hu8cSGtj3fD1","executionInfo":{"status":"ok","timestamp":1658348529265,"user_tz":300,"elapsed":13,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"93986521-5c1a-47a4-e57b-b0e352d425c4"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 77.82152230971128,\n"," 'f1': 0.7758810170952618,\n"," 'precision': 0.7807522349051432,\n"," 'recall': 0.7782152230971129}"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["# Compare model_5 results to baseline\n","compare_baseline_to_new_results(baseline_results, model_5_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5uRQyr43wUF","executionInfo":{"status":"ok","timestamp":1658348529266,"user_tz":300,"elapsed":11,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"2c8bb4cb-1d16-41fa-86e8-042f604e8832"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 77.82, Difference: -1.44\n","Baseline precision: 0.81, New precision: 0.78, Difference: -0.03\n","Baseline recall: 0.79, New recall: 0.78, Difference: -0.01\n","Baseline f1: 0.79, New f1: 0.78, Difference: -0.01\n"]}]},{"cell_type":"markdown","source":["## Using Pretained embeddings (transfer learning!)\n","\n","We're going to be using the [Universal Sentence Encoder](https://www.aclweb.org/anthology/D18-2029.pdf) from [TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4) (a great resource containing a plethora of pretrained model resources for a variety of tasks).\n","\n","There are many different pretrained text embedding options on TensorFlow Hub, however, some require different levels of text preprocessing than others. Best to experiment with a few and see which best suits your use case."],"metadata":{"id":"sCSyLv8P4CJl"}},{"cell_type":"markdown","source":["### Model_6: TensorFlow Hub Pretrained Sentence Encoder\n","\n","The `Universal Sentence Encoder` creates an embedding at a whole-sentence level, compared to the embedding layer we made which was word-level. \n","\n","Our embedding layer also outputs an a 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence.\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-USE-tensorflow-hub-encoder-decoder-model.png)\n","*The feature extractor model we're building through the eyes of an **encoder/decoder** model.*\n","\n","We can load in a TensorFlow Hub module using the [`hub.load()`](https://www.tensorflow.org/hub/api_docs/python/hub/load) method and passing it the target URL of the module we'd like to use, in our case, it's \"https://tfhub.dev/google/universal-sentence-encoder/4\".\n","\n","Let's load the Universal Sentence Encoder model and test it on a couple of sentences."],"metadata":{"id":"4PtPWx604RNB"}},{"cell_type":"code","source":["# Example of a pretrained embedding wit huniversal sentence encoder\n","import tensorflow_hub as hub\n","sample_sentence = \"There's a flood in my street!\"\n","\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n","embed_samples = embed([sample_sentence,\n","                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n","\n","print(embed_samples[0][:50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyMYelIT6PFB","executionInfo":{"status":"ok","timestamp":1658348555984,"user_tz":300,"elapsed":26727,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"0b0e305b-ed03-4911-f3b2-c8a7e532cc2d"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[-0.01157028  0.0248591   0.02878048 -0.012715    0.03971538  0.0882776\n","  0.02680984  0.05589836 -0.0106873  -0.00597291  0.00639323 -0.01819518\n","  0.00030813  0.09105888  0.05874644 -0.03180628  0.01512474 -0.05162929\n","  0.00991367 -0.06865347 -0.04209306  0.02678981  0.03011006  0.00321069\n"," -0.00337973 -0.04787357  0.0226672  -0.00985925 -0.04063613 -0.01292092\n"," -0.04666384  0.05630299 -0.03949255  0.00517686  0.02495829 -0.0701444\n","  0.02871508  0.04947684 -0.00633979 -0.08960192  0.02807118 -0.00808364\n"," -0.01360602  0.0599865  -0.10361787 -0.05195374  0.00232954 -0.02332531\n"," -0.03758105  0.03327728], shape=(50,), dtype=float32)\n"]}]},{"cell_type":"code","source":["embed_samples[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxMwIuDl62kR","executionInfo":{"status":"ok","timestamp":1658348555985,"user_tz":300,"elapsed":6,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"742cb225-d875-4589-f213-95308b4317fd"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([512])"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","source":["When we pass a sentence toi the Unisersal Sentence Encoder (USE) they are encoded from strings into 512 dimensional vectors.\n","\n","Let's build one!\n","\n","We can convert the TensorFlow Hub USE module into a Keras layer using the [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) class."],"metadata":{"id":"fTBHWVSQ98lv"}},{"cell_type":"code","source":["# We can use this encoding layer in place of our text_vectorizer and embedding layer\n","sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n","                                        input_shape=[], # shape of inputs coming to our model \n","                                        dtype=tf.string, # data type of inputs coming to the USE layer\n","                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n","                                        name=\"USE\") "],"metadata":{"id":"zav29Fdt-m5s","executionInfo":{"status":"ok","timestamp":1658348559425,"user_tz":300,"elapsed":3442,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":["Now we've got the USE as a Keras layer, we can use it in a Keras Sequential model."],"metadata":{"id":"4L1MNbHa-wH6"}},{"cell_type":"code","source":["# Create a model using the Sequential API\n","model_6 = tf.keras.Sequential([\n","                               sentence_encoder_layer,\n","                               layers.Dense(64, activation=\"relu\"),\n","                               layers.Dense(1, activation=\"sigmoid\")\n","], name=\"model_6_USE\")\n","\n","# Compile model\n","model_6.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","model_6.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Za8vwjih-1bT","executionInfo":{"status":"ok","timestamp":1658348560152,"user_tz":300,"elapsed":731,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"9fde40a7-100a-4f11-8621-e178f7d009cb"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6_USE\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," USE (KerasLayer)            (None, 512)               256797824 \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                32832     \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 256,830,721\n","Trainable params: 32,897\n","Non-trainable params: 256,797,824\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Note the trainable parameters are only in the dense layers that we created."],"metadata":{"id":"4hJRQEH2AmHN"}},{"cell_type":"code","source":["# Train a classifier on top of the pretrained embeddings\n","model_6_history = model_6.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n","                                                                     \"tf_hub_sentence_encoder\")])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UX6lKmWcA9QS","executionInfo":{"status":"ok","timestamp":1658348578159,"user_tz":300,"elapsed":18011,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"a9c382d2-c777-4ccd-82d2-49ef907570a8"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20220720-202240\n","Epoch 1/5\n","215/215 [==============================] - 6s 13ms/step - loss: 0.5008 - accuracy: 0.7892 - val_loss: 0.4478 - val_accuracy: 0.7966\n","Epoch 2/5\n","215/215 [==============================] - 3s 12ms/step - loss: 0.4144 - accuracy: 0.8133 - val_loss: 0.4369 - val_accuracy: 0.8058\n","Epoch 3/5\n","215/215 [==============================] - 2s 11ms/step - loss: 0.3998 - accuracy: 0.8212 - val_loss: 0.4329 - val_accuracy: 0.8110\n","Epoch 4/5\n","215/215 [==============================] - 4s 19ms/step - loss: 0.3925 - accuracy: 0.8266 - val_loss: 0.4288 - val_accuracy: 0.8110\n","Epoch 5/5\n","215/215 [==============================] - 3s 15ms/step - loss: 0.3860 - accuracy: 0.8276 - val_loss: 0.4309 - val_accuracy: 0.8123\n"]}]},{"cell_type":"code","source":["# Make predictions with USE TF Hub model\n","model_6_pred_probs = model_6.predict(val_sentences)\n","model_6_pred_probs[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6GgEZVPxBM9C","executionInfo":{"status":"ok","timestamp":1658348579436,"user_tz":300,"elapsed":1284,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"58679c5f-eaf7-4f58-d083-472f249c8667"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.14443196],\n","       [0.72715044],\n","       [0.9856655 ],\n","       [0.19740927],\n","       [0.73417014],\n","       [0.6859662 ],\n","       [0.9808888 ],\n","       [0.97411025],\n","       [0.9157322 ],\n","       [0.08070081]], dtype=float32)"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["# Convert to label predictions\n","model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n","model_6_preds[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KIZve73BVhc","executionInfo":{"status":"ok","timestamp":1658348579437,"user_tz":300,"elapsed":8,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"a6954ef1-c6df-4a95-b3d6-e3e1537a8681"},"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["# Calculate performance metrics\n","model_6_results = calculate_results(val_labels, model_6_preds)\n","model_6_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BYXuibQBrqT","executionInfo":{"status":"ok","timestamp":1658348579438,"user_tz":300,"elapsed":7,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"d7ba9aa6-224f-48aa-c8bc-d4619512e7dd"},"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 81.23359580052494,\n"," 'f1': 0.810686575717776,\n"," 'precision': 0.8148798668657973,\n"," 'recall': 0.8123359580052494}"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["# Compare to baseline\n","compare_baseline_to_new_results(baseline_results, model_6_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6Lh7urNEFTz","executionInfo":{"status":"ok","timestamp":1658348579438,"user_tz":300,"elapsed":6,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"0fc2ae93-79f7-424e-dc13-691737656517"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 81.23, Difference: 1.97\n","Baseline precision: 0.81, New precision: 0.81, Difference: 0.00\n","Baseline recall: 0.79, New recall: 0.81, Difference: 0.02\n","Baseline f1: 0.79, New f1: 0.81, Difference: 0.02\n"]}]},{"cell_type":"markdown","source":["### Model_7: TensorFlow Hub pretrained senence encoder, trained on 10% of training data\n","\n","So we see just pulling in the pretrained weights has alright performance, but one of the benefits is to boost it further with minimal training on our custom data"],"metadata":{"id":"rzDyGsWuEJee"}},{"cell_type":"code","source":["# Create subset data\n","train_sentences_90_pct, train_sentences_10_pct, train_labels_90_pct, train_labels_10_pct = train_test_split(np.array(train_sentences),\n","                                                                                                            train_labels,\n","                                                                                                            test_size=0.1,\n","                                                                                                            random_state=42)"],"metadata":{"id":"XYAsWV0OEp71","executionInfo":{"status":"ok","timestamp":1658348579561,"user_tz":300,"elapsed":6,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["# check length of training datasets\n","print(f\"total length of training examples: {len(train_sentences)}\")\n","print(f\"length of 10% training examples: {len(train_labels_10_pct)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbRVQNzoFrB2","executionInfo":{"status":"ok","timestamp":1658348579561,"user_tz":300,"elapsed":4,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"63da708e-b75a-4d16-a84a-a0d2e2c3e4f2"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["total length of training examples: 6851\n","length of 10% training examples: 686\n"]}]},{"cell_type":"markdown","source":["Let's check to make sure that the random sampling of our training data is still mostly balanced..."],"metadata":{"id":"z370jOdQGCBv"}},{"cell_type":"code","source":["# Check training label counts\n","pd.Series(train_labels_10_pct).value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzRNnwF6Gftq","executionInfo":{"status":"ok","timestamp":1658348579561,"user_tz":300,"elapsed":3,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"7fb0f2f5-e653-444f-f11e-49be932747e9"},"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    415\n","1    271\n","dtype: int64"]},"metadata":{},"execution_count":95}]},{"cell_type":"markdown","source":["We will create model_7 from a copy of model_6 using `tf.keras.model.clone_model()`. This will keep the same architecture, but reset the weights."],"metadata":{"id":"eSjHXpa6Gm5z"}},{"cell_type":"code","source":["# Clone model_6\n","model_7 = tf.keras.models.clone_model(model_6)\n","\n","# Compile model\n","model_7.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","# Summary\n","model_7.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1N9Th_a6HK4z","executionInfo":{"status":"ok","timestamp":1658348586035,"user_tz":300,"elapsed":6477,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"37646625-52b7-42c0-b74a-f51bd359d13e"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6_USE\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," USE (KerasLayer)            (None, 512)               256797824 \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                32832     \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 256,830,721\n","Trainable params: 32,897\n","Non-trainable params: 256,797,824\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Fit the model_7 to the 10% training data\n","model_7_history = model_7.fit(x=train_sentences_10_pct,\n","                              y=train_labels_10_pct,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n","                                                                     \"10_percent_tf_hub_sentence_encoder\")])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbK1EtFlHTes","executionInfo":{"status":"ok","timestamp":1658348598473,"user_tz":300,"elapsed":12445,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"f0f9a80d-1f76-4db4-894f-a6ea53038547"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/10_percent_tf_hub_sentence_encoder/20220720-202306\n","Epoch 1/5\n","22/22 [==============================] - 8s 131ms/step - loss: 0.6716 - accuracy: 0.6574 - val_loss: 0.6526 - val_accuracy: 0.6903\n","Epoch 2/5\n","22/22 [==============================] - 1s 56ms/step - loss: 0.5972 - accuracy: 0.8032 - val_loss: 0.5944 - val_accuracy: 0.7362\n","Epoch 3/5\n","22/22 [==============================] - 1s 42ms/step - loss: 0.5178 - accuracy: 0.8149 - val_loss: 0.5398 - val_accuracy: 0.7625\n","Epoch 4/5\n","22/22 [==============================] - 1s 44ms/step - loss: 0.4526 - accuracy: 0.8265 - val_loss: 0.5084 - val_accuracy: 0.7677\n","Epoch 5/5\n","22/22 [==============================] - 1s 51ms/step - loss: 0.4094 - accuracy: 0.8382 - val_loss: 0.4915 - val_accuracy: 0.7703\n"]}]},{"cell_type":"code","source":["# Make predictions, turn into labels\n","model_7_pred_probs = model_7.predict(val_sentences)\n","model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))"],"metadata":{"id":"DsU_TznoH2vp","executionInfo":{"status":"ok","timestamp":1658348599531,"user_tz":300,"elapsed":1073,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["# Results\n","model_7_results = calculate_results(val_labels, model_7_preds)\n","model_7_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quT0LiGhJzk6","executionInfo":{"status":"ok","timestamp":1658348599531,"user_tz":300,"elapsed":16,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"77a2b68c-15cd-4f5b-a3d9-d3bededa219a"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 77.03412073490814,\n"," 'f1': 0.7667059443150692,\n"," 'precision': 0.7755630249535594,\n"," 'recall': 0.7703412073490814}"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["# Compare to baseline\n","compare_baseline_to_new_results(baseline_results, model_7_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JdvI9r7sKINF","executionInfo":{"status":"ok","timestamp":1658348599532,"user_tz":300,"elapsed":14,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"85f22224-0e46-4c6f-ae8e-a7db0232cc0a"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 77.03, Difference: -2.23\n","Baseline precision: 0.81, New precision: 0.78, Difference: -0.04\n","Baseline recall: 0.79, New recall: 0.77, Difference: -0.02\n","Baseline f1: 0.79, New f1: 0.77, Difference: -0.02\n"]}]},{"cell_type":"markdown","source":["## Compare the performance of each of our models\n","\n","We made a lot of models! Let's compare them!\n","\n","*Note: This process of creating numerous models and then comparing them is pretty standard.\n","\n","We will visualize our model performances using a pandas DataFrame."],"metadata":{"id":"Oo9laiI4KUwa"}},{"cell_type":"code","source":["# Combike model results into a DataFrame\n","all_model_results = pd.DataFrame({\"baseline:\": baseline_results,\n","                                  \"simple_dense\": model_1_results,\n","                                  \"lstm\": model_2_results,\n","                                  \"gru\": model_3_results,\n","                                  \"bidirectional\": model_4_results,\n","                                  \"conv1d\": model_5_results,\n","                                  \"tf_hub_sentence_encoder\": model_6_results,\n","                                  \"tf_hub_10_percent_data\": model_7_results})\n","all_model_results = all_model_results.transpose()\n","all_model_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"lQVNoaQTLdIO","executionInfo":{"status":"ok","timestamp":1658348599533,"user_tz":300,"elapsed":11,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"fe757b18-48ee-4628-bb52-882d6e9f7806"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                          accuracy  precision    recall        f1\n","baseline:                79.265092   0.811139  0.792651  0.786219\n","simple_dense             78.740157   0.791492  0.787402  0.784697\n","lstm                     75.065617   0.751008  0.750656  0.748927\n","gru                      76.771654   0.767545  0.767717  0.766793\n","bidirectional            76.640420   0.766590  0.766404  0.765121\n","conv1d                   77.821522   0.780752  0.778215  0.775881\n","tf_hub_sentence_encoder  81.233596   0.814880  0.812336  0.810687\n","tf_hub_10_percent_data   77.034121   0.775563  0.770341  0.766706"],"text/html":["\n","  <div id=\"df-026eb181-1aeb-4d65-8c8e-4bed4471bb3d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline:</th>\n","      <td>79.265092</td>\n","      <td>0.811139</td>\n","      <td>0.792651</td>\n","      <td>0.786219</td>\n","    </tr>\n","    <tr>\n","      <th>simple_dense</th>\n","      <td>78.740157</td>\n","      <td>0.791492</td>\n","      <td>0.787402</td>\n","      <td>0.784697</td>\n","    </tr>\n","    <tr>\n","      <th>lstm</th>\n","      <td>75.065617</td>\n","      <td>0.751008</td>\n","      <td>0.750656</td>\n","      <td>0.748927</td>\n","    </tr>\n","    <tr>\n","      <th>gru</th>\n","      <td>76.771654</td>\n","      <td>0.767545</td>\n","      <td>0.767717</td>\n","      <td>0.766793</td>\n","    </tr>\n","    <tr>\n","      <th>bidirectional</th>\n","      <td>76.640420</td>\n","      <td>0.766590</td>\n","      <td>0.766404</td>\n","      <td>0.765121</td>\n","    </tr>\n","    <tr>\n","      <th>conv1d</th>\n","      <td>77.821522</td>\n","      <td>0.780752</td>\n","      <td>0.778215</td>\n","      <td>0.775881</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_sentence_encoder</th>\n","      <td>81.233596</td>\n","      <td>0.814880</td>\n","      <td>0.812336</td>\n","      <td>0.810687</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_10_percent_data</th>\n","      <td>77.034121</td>\n","      <td>0.775563</td>\n","      <td>0.770341</td>\n","      <td>0.766706</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-026eb181-1aeb-4d65-8c8e-4bed4471bb3d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-026eb181-1aeb-4d65-8c8e-4bed4471bb3d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-026eb181-1aeb-4d65-8c8e-4bed4471bb3d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["# Convert accuracy to decimal\n","all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100"],"metadata":{"id":"4i_R_NPKL71Z","executionInfo":{"status":"ok","timestamp":1658348599533,"user_tz":300,"elapsed":8,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["# Plot all results\n","all_model_results.plot(kind=\"bar\").legend(bbox_to_anchor=(1.0,1.0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"cP5qQrNGMVLn","executionInfo":{"status":"ok","timestamp":1658348600028,"user_tz":300,"elapsed":502,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"d9c1a84b-0024-43b8-c06c-e78e172ec2a1"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f2b70b652d0>"]},"metadata":{},"execution_count":103},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["We can see that the model that used the pretrained USE weights perfoms the best. Even the 10% version performs better than the others.\n","\n","Drill-down to the F1 scores only"],"metadata":{"id":"lQRmVC2eMZk_"}},{"cell_type":"code","source":["# Sort models by F1 score\n","all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10,7))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"DNx_DEgfMxUD","executionInfo":{"status":"ok","timestamp":1658348600450,"user_tz":300,"elapsed":426,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"18fda3a8-1504-49af-da84-e72657197628"},"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f2b4603edd0>"]},"metadata":{},"execution_count":104},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"CdHg74lnWK3s"}},{"cell_type":"markdown","source":["Can also visualize using TensorBoard"],"metadata":{"id":"saLWvxgiM5sr"}},{"cell_type":"code","source":["# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n","# # Upload TensorBoard dev records\n","# !tensorboard dev upload --logdir ./model_logs \\\n","#   --name \"NLP modelling experiments\" \\\n","#   --description \"A series of different NLP modellings experiments with various models\" \\\n","#   --one_shot # exits the uploader when upload has finished"],"metadata":{"id":"j6oSPAD7NJ0o","executionInfo":{"status":"ok","timestamp":1658348600450,"user_tz":300,"elapsed":8,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["# To remove previous experiments, you can do so using the following command\n","# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"],"metadata":{"id":"lE8uaJ6WNKEC","executionInfo":{"status":"ok","timestamp":1658348600450,"user_tz":300,"elapsed":7,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":["## Combining our models (creating an `ensemble` model)\n","\n","Many neural networks in production models use an **ensemble** (aka a combination) of models to make predictions.\n","\n","The macro idea is that if several uncorrelated models agree on a prediction, that the prediction is more likely to be true. \n","\n","In our case uncorrelated does not mean that they are trained on unrelated data, but that they are structured differenlty for pattern recognition. \n","\n","\n","**Ways of combining our models**:\n","1. *Averaging*: Take the output prediction probabilities of each model for each sample and combine and then average\n","2. *Majority Vote*: The most chosen class\n","3. *Model Stacking*: Take the outputs of each of the models and use them as inputs to another model\n","\n","(These methods have been adapted from Ch. 6 of the [Machine Learning Engineering Book](http://www.mlebook.com/wiki/doku.php) by Andriy Burkov.\n","\n","We are going to combine the baseline model (`model_0`) with the LSTM model (`model_2`) and the 100% USE model (`model_6`) through averaging the combined prediction probabilities.  "],"metadata":{"id":"Mf6Wj8MeNMVU"}},{"cell_type":"code","source":["# Calcualte mean pred_probs for the chosen models\n","baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model\n","combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs)\n","combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes\n","combined_preds[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7EUp86fHTEzY","executionInfo":{"status":"ok","timestamp":1658348600451,"user_tz":300,"elapsed":8,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"35f05216-cd28-4501-f5c9-54200b26a3fd"},"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n","       0., 0., 1.], dtype=float32)>"]},"metadata":{},"execution_count":107}]},{"cell_type":"code","source":["# Calculate ensemble predictions\n","ensemble_results = calculate_results(val_labels, combined_preds)\n","ensemble_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTJqKlr5Y1Vn","executionInfo":{"status":"ok","timestamp":1658348600451,"user_tz":300,"elapsed":7,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"4038465e-64d4-4df1-8685-b74aced2a43f"},"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 78.08398950131233,\n"," 'f1': 0.7805169025578647,\n"," 'precision': 0.7805216999297674,\n"," 'recall': 0.7808398950131233}"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["# Let's add this to our DataFrame so that we can compare to our single model results more easily\n","all_model_results.loc[\"ensemble_results\"] = ensemble_results\n","all_model_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"id":"Gu7wKlsDZBuy","executionInfo":{"status":"ok","timestamp":1658348600451,"user_tz":300,"elapsed":5,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"aed7c039-a537-448a-ad3a-4c7ca974ac24"},"execution_count":109,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                          accuracy  precision    recall        f1\n","baseline:                 0.792651   0.811139  0.792651  0.786219\n","simple_dense              0.787402   0.791492  0.787402  0.784697\n","lstm                      0.750656   0.751008  0.750656  0.748927\n","gru                       0.767717   0.767545  0.767717  0.766793\n","bidirectional             0.766404   0.766590  0.766404  0.765121\n","conv1d                    0.778215   0.780752  0.778215  0.775881\n","tf_hub_sentence_encoder   0.812336   0.814880  0.812336  0.810687\n","tf_hub_10_percent_data    0.770341   0.775563  0.770341  0.766706\n","ensemble_results         78.083990   0.780522  0.780840  0.780517"],"text/html":["\n","  <div id=\"df-c4cc092b-c15a-4ecb-b1d9-e973f840385c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline:</th>\n","      <td>0.792651</td>\n","      <td>0.811139</td>\n","      <td>0.792651</td>\n","      <td>0.786219</td>\n","    </tr>\n","    <tr>\n","      <th>simple_dense</th>\n","      <td>0.787402</td>\n","      <td>0.791492</td>\n","      <td>0.787402</td>\n","      <td>0.784697</td>\n","    </tr>\n","    <tr>\n","      <th>lstm</th>\n","      <td>0.750656</td>\n","      <td>0.751008</td>\n","      <td>0.750656</td>\n","      <td>0.748927</td>\n","    </tr>\n","    <tr>\n","      <th>gru</th>\n","      <td>0.767717</td>\n","      <td>0.767545</td>\n","      <td>0.767717</td>\n","      <td>0.766793</td>\n","    </tr>\n","    <tr>\n","      <th>bidirectional</th>\n","      <td>0.766404</td>\n","      <td>0.766590</td>\n","      <td>0.766404</td>\n","      <td>0.765121</td>\n","    </tr>\n","    <tr>\n","      <th>conv1d</th>\n","      <td>0.778215</td>\n","      <td>0.780752</td>\n","      <td>0.778215</td>\n","      <td>0.775881</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_sentence_encoder</th>\n","      <td>0.812336</td>\n","      <td>0.814880</td>\n","      <td>0.812336</td>\n","      <td>0.810687</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_10_percent_data</th>\n","      <td>0.770341</td>\n","      <td>0.775563</td>\n","      <td>0.770341</td>\n","      <td>0.766706</td>\n","    </tr>\n","    <tr>\n","      <th>ensemble_results</th>\n","      <td>78.083990</td>\n","      <td>0.780522</td>\n","      <td>0.780840</td>\n","      <td>0.780517</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4cc092b-c15a-4ecb-b1d9-e973f840385c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c4cc092b-c15a-4ecb-b1d9-e973f840385c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c4cc092b-c15a-4ecb-b1d9-e973f840385c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":109}]},{"cell_type":"markdown","source":["Well it looks like our models are all very close. We did only train for 5 epochs on each...\n","\n","But it is also good after training for a bit more to revisit the data if many moidels are performing very closely."],"metadata":{"id":"c_3IUk_PZW0R"}},{"cell_type":"markdown","source":["## Saving and loading the trained model\n","\n","Always good practice to save your trained models to avoid needing to retrain them. It also allows for export and use elsewhere.\n","\n","There are two main ways of [saving a model in TensorFlow](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model):\n","1. The `HDF5` format. \n","2. The `SavedModel` format (default).\n","\n","We will look at both\n"],"metadata":{"id":"Yz21MtgaZaiG"}},{"cell_type":"code","source":["# Save TF Hub Sentence Encoder model to HDF5 Format\n","model_6.save(\"model_6.h5\")"],"metadata":{"id":"-nE19SF7yJQp","executionInfo":{"status":"ok","timestamp":1658348605337,"user_tz":300,"elapsed":4890,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":["When loading back in a file saves as HDF5 you *need to let TensorFlow know about any custom objects*. For example, we used TensorFlow Hub which isn't part of the pure TensorFlow package."],"metadata":{"id":"IbRW4FEky8sR"}},{"cell_type":"code","source":["loaded_model_6 = tf.keras.models.load_model(\"model_6.h5\",\n","                                            custom_objects={\"KerasLayer\": hub.KerasLayer})"],"metadata":{"id":"bKqnPib3zMnV","executionInfo":{"status":"ok","timestamp":1658348658349,"user_tz":300,"elapsed":5268,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["loaded_model_6.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2afkfIoRg7I","executionInfo":{"status":"ok","timestamp":1658348667638,"user_tz":300,"elapsed":157,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"46eec26a-ff55-432d-fd2b-820fbadd573d"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6_USE\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," USE (KerasLayer)            (None, 512)               256797824 \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                32832     \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 256,830,721\n","Trainable params: 32,897\n","Non-trainable params: 256,797,824\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["loaded_model_6.evaluate(val_sentences, val_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vl1HvcdhRzu3","executionInfo":{"status":"ok","timestamp":1658348689863,"user_tz":300,"elapsed":1268,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"eb9b5ef4-c584-482a-9f3c-cf031b323e34"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 1s 18ms/step - loss: 0.4309 - accuracy: 0.8123\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.43088313937187195, 0.8123359680175781]"]},"metadata":{},"execution_count":119}]},{"cell_type":"markdown","source":["When calling the `save()` method and passaing a filepath we can save our model in the `SavedModel` format"],"metadata":{"id":"tDtOGEp1SNQX"}},{"cell_type":"code","source":["# Save TF Hub Sentence Encoder model to SavedModel format\n","model_6.save(\"model_6_SavedModel_format\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYa8ifkwSa8a","executionInfo":{"status":"ok","timestamp":1658348782798,"user_tz":300,"elapsed":10729,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"9b3aeaed-4817-48e5-a405-91c0c3708477"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) USE_input with unsupported characters which will be renamed to use_input in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: model_6_SavedModel_format/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: model_6_SavedModel_format/assets\n"]}]},{"cell_type":"markdown","source":["When using the `SavedModel` format you don't need to call specific custom objects."],"metadata":{"id":"sdi_JYwEShnv"}},{"cell_type":"code","source":["# Load SavedModel\n","loaded_model_6_SavedModel = tf.keras.models.load_model(\"model_6_SavedModel_format\")"],"metadata":{"id":"9aoN4O9ASpm1","executionInfo":{"status":"ok","timestamp":1658348847781,"user_tz":300,"elapsed":6832,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["# Evaluate the loaded model\n","loaded_model_6_SavedModel.evaluate(val_sentences, val_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojCnLkXNSybq","executionInfo":{"status":"ok","timestamp":1658348875790,"user_tz":300,"elapsed":2229,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"51e39ac2-0d5f-4c1e-880e-2bff38cd1308"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 2s 18ms/step - loss: 0.4309 - accuracy: 0.8123\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.43088313937187195, 0.8123359680175781]"]},"metadata":{},"execution_count":122}]},{"cell_type":"markdown","source":["Same performance between saves! So... why use one over the other?\n","\n","Generally the `SavedModel` will be best, but this is a TensorFlow standard. If you need a more general-purpose data standard then `HDF5` might be better."],"metadata":{"id":"aLul5XDZTAnf"}},{"cell_type":"markdown","source":["## Finding the most wrong predictions\n","\n","As highlighted before if all our models are returning similar performance it is a good idea to dig into the data.\n","\n","One of the best ways of exploring data is to see what predictions are the most wrong. Let's *visualize* this.\n","\n","* If our best model still isn't perfect, what examples is it getting wrong?\n","* Which ones are the most wrong?\n","* Are there some labels which are wrong? E.g. the model gets it right but the ground truth label doesn't reflect this"],"metadata":{"id":"dsLNsmzfTSa5"}},{"cell_type":"code","source":["# Create DataFrame wqith validation sentences and best performing model predictions\n","val_df = pd.DataFrame({\"text\": val_sentences,\n","                       \"target\": val_labels,\n","                       \"pred\": model_6_preds,\n","                       \"pred_prob\": tf.squeeze(model_6_pred_probs)})\n","val_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"P6LYY1V8T2ZF","executionInfo":{"status":"ok","timestamp":1658349190185,"user_tz":300,"elapsed":149,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"61ee4f1d-aab8-40cf-d62a-839f1022880d"},"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  target  pred  pred_prob\n","0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0   0.0   0.144432\n","1  FedEx no longer to transport bioterror germs i...       0   1.0   0.727150\n","2  Gunmen kill four in El Salvador bus attack: Su...       1   1.0   0.985666\n","3  @camilacabello97 Internally and externally scr...       1   0.0   0.197409\n","4  Radiation emergency #preparedness starts with ...       1   1.0   0.734170"],"text/html":["\n","  <div id=\"df-41725532-5112-4d0e-8663-740089d6b908\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>pred</th>\n","      <th>pred_prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.144432</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FedEx no longer to transport bioterror germs i...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.727150</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.985666</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@camilacabello97 Internally and externally scr...</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.197409</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Radiation emergency #preparedness starts with ...</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.734170</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41725532-5112-4d0e-8663-740089d6b908')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-41725532-5112-4d0e-8663-740089d6b908 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-41725532-5112-4d0e-8663-740089d6b908');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":125}]},{"cell_type":"markdown","source":["Now let's find the wrong preditions, and sort"],"metadata":{"id":"LIW6x-5NTd84"}},{"cell_type":"code","source":["# Find wrong predictions and sort by pred_prob\n","most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False)\n","most_wrong[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"97-KgQ_ZUPH6","executionInfo":{"status":"ok","timestamp":1658349378005,"user_tz":300,"elapsed":133,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"6123e113-dac3-4caf-bc78-bee90bc7d137"},"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  text  target  pred  \\\n","31   ? High Skies - Burning Buildings ? http://t.co...       0   1.0   \n","759  FedEx will no longer transport bioterror patho...       0   1.0   \n","209  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0   1.0   \n","393  @SonofLiberty357 all illuminated by the bright...       0   1.0   \n","628  @noah_anyname That's where the concentration c...       0   1.0   \n","49   @madonnamking RSPCA site multiple 7 story high...       0   1.0   \n","109  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...       0   1.0   \n","251  @AshGhebranious civil rights continued in the ...       0   1.0   \n","698  åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...       0   1.0   \n","144                                 The Sound of Arson       0   1.0   \n","695  A look at state actions a year after Ferguson'...       0   1.0   \n","567  @RebeccaforReal accepts Wisconsin Emergency Re...       0   1.0   \n","549  Day 2. Liquidation of emergency at chemical ob...       0   1.0   \n","381                    Deaths 3 http://t.co/nApviyGKYK       0   1.0   \n","119  @freefromwolves GodsLove &amp; #thankU brother...       0   1.0   \n","1    FedEx no longer to transport bioterror germs i...       0   1.0   \n","344  Air Group is here to the rescue! We have 24/7 ...       0   1.0   \n","397  The #tubestrike is because TFL workers may hav...       0   1.0   \n","760  Crack in the path where I wiped out this morni...       0   1.0   \n","303  Trafford Centre film fans angry after Odeon ci...       0   1.0   \n","\n","     pred_prob  \n","31    0.910481  \n","759   0.864676  \n","209   0.837961  \n","393   0.836361  \n","628   0.835225  \n","49    0.834875  \n","109   0.800890  \n","251   0.782611  \n","698   0.782433  \n","144   0.771343  \n","695   0.770492  \n","567   0.752019  \n","549   0.739145  \n","381   0.738355  \n","119   0.736023  \n","1     0.727150  \n","344   0.723794  \n","397   0.707823  \n","760   0.702342  \n","303   0.690299  "],"text/html":["\n","  <div id=\"df-7997e484-3df6-4831-b1f5-b9414ba114f6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>pred</th>\n","      <th>pred_prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31</th>\n","      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.910481</td>\n","    </tr>\n","    <tr>\n","      <th>759</th>\n","      <td>FedEx will no longer transport bioterror patho...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.864676</td>\n","    </tr>\n","    <tr>\n","      <th>209</th>\n","      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.837961</td>\n","    </tr>\n","    <tr>\n","      <th>393</th>\n","      <td>@SonofLiberty357 all illuminated by the bright...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.836361</td>\n","    </tr>\n","    <tr>\n","      <th>628</th>\n","      <td>@noah_anyname That's where the concentration c...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.835225</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>@madonnamking RSPCA site multiple 7 story high...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.834875</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.800890</td>\n","    </tr>\n","    <tr>\n","      <th>251</th>\n","      <td>@AshGhebranious civil rights continued in the ...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.782611</td>\n","    </tr>\n","    <tr>\n","      <th>698</th>\n","      <td>åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.782433</td>\n","    </tr>\n","    <tr>\n","      <th>144</th>\n","      <td>The Sound of Arson</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.771343</td>\n","    </tr>\n","    <tr>\n","      <th>695</th>\n","      <td>A look at state actions a year after Ferguson'...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.770492</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>@RebeccaforReal accepts Wisconsin Emergency Re...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.752019</td>\n","    </tr>\n","    <tr>\n","      <th>549</th>\n","      <td>Day 2. Liquidation of emergency at chemical ob...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.739145</td>\n","    </tr>\n","    <tr>\n","      <th>381</th>\n","      <td>Deaths 3 http://t.co/nApviyGKYK</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.738355</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>@freefromwolves GodsLove &amp;amp; #thankU brother...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.736023</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FedEx no longer to transport bioterror germs i...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.727150</td>\n","    </tr>\n","    <tr>\n","      <th>344</th>\n","      <td>Air Group is here to the rescue! We have 24/7 ...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.723794</td>\n","    </tr>\n","    <tr>\n","      <th>397</th>\n","      <td>The #tubestrike is because TFL workers may hav...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.707823</td>\n","    </tr>\n","    <tr>\n","      <th>760</th>\n","      <td>Crack in the path where I wiped out this morni...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.702342</td>\n","    </tr>\n","    <tr>\n","      <th>303</th>\n","      <td>Trafford Centre film fans angry after Odeon ci...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.690299</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7997e484-3df6-4831-b1f5-b9414ba114f6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7997e484-3df6-4831-b1f5-b9414ba114f6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7997e484-3df6-4831-b1f5-b9414ba114f6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":127}]},{"cell_type":"markdown","source":["Let's write some code to visualize the sample text, truth label, prediction class, and prediction probability.\n","\n","Note:\n","\n","0 = Not a real disaster\n","1 = Real disaster"],"metadata":{"id":"Lo1sN5FiU0bo"}},{"cell_type":"code","source":["# Check the false positives (model predicted 1 when should've been 0)\n","for row in most_wrong[:10].itertuples(): # loop through the top 10 rows (change the index to view different rows)\n","  _, text, target, pred, prob = row\n","  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"----\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEF3H2V1Vs6U","executionInfo":{"status":"ok","timestamp":1658349617223,"user_tz":300,"elapsed":131,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"86d1dba0-2421-46f7-b583-e05a6d1e73e3"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 0, Pred: 1, Prob: 0.9104807376861572\n","Text:\n","? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8646755218505859\n","Text:\n","FedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8379610776901245\n","Text:\n","Ashes 2015: AustraliaÛªs collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8363614082336426\n","Text:\n","@SonofLiberty357 all illuminated by the brightly burning buildings all around the town!\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8352250456809998\n","Text:\n","@noah_anyname That's where the concentration camps and mass murder come in. \n"," \n","EVERY. FUCKING. TIME.\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8348746299743652\n","Text:\n","@madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8008897304534912\n","Text:\n","[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.782611072063446\n","Text:\n","@AshGhebranious civil rights continued in the 60s. And what about trans-generational trauma? if anything we should listen to the Americans.\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.7824334502220154\n","Text:\n","åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Tent Collapse Story: Correction: Tent Collapse story åÈ http://t.co/fDJUYvZMrv @wizkidayo\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.7713426947593689\n","Text:\n","The Sound of Arson\n","\n","----\n","\n"]}]},{"cell_type":"markdown","source":["View the bottom of the `most_wrong` dataframe to inspect false negatives"],"metadata":{"id":"qY6vwQcVVuw8"}},{"cell_type":"code","source":["# Check the most wrong false negatives (model predicted 0 when should've predict 1)\n","for row in most_wrong[-10:].itertuples():\n","  _, text, target, pred, prob = row\n","  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"----\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gT_X62UwWDKM","executionInfo":{"status":"ok","timestamp":1658349700880,"user_tz":300,"elapsed":142,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"2c22a320-9aa0-445e-901c-23ab3d00ca9f"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 1, Pred: 0, Prob: 0.06304335594177246\n","Text:\n","@BoyInAHorsemask its a panda trapped in a dogs body\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.06279505044221878\n","Text:\n","going to redo my nails and watch behind the scenes of desolation of smaug ayyy\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.06060810014605522\n","Text:\n","VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH 241487 http://t.co/yFy3nkkcoH http://t.co/KNEhVvOHVK\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.0573178268969059\n","Text:\n","@willienelson We need help! Horses will die!Please RT &amp; sign petition!Take a stand &amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.045355550944805145\n","Text:\n","You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.041451383382081985\n","Text:\n","I get to smoke my shit in peace\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.03926115110516548\n","Text:\n","@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.0385933518409729\n","Text:\n","Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.0362723171710968\n","Text:\n","Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.03288796916604042\n","Text:\n","Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n","\n","----\n","\n"]}]},{"cell_type":"markdown","source":["Hmmm... looks like some of our labels might not be right!!"],"metadata":{"id":"-Rx61uVjWEWF"}},{"cell_type":"markdown","source":["## Making predictions on the test dataset.\n","\n","Since these don't have labels we will need to inspect them ourselves to gauge performance\n"],"metadata":{"id":"sv_obGdKWYal"}},{"cell_type":"code","source":["# Prediction on test_dataset examples\n","\n","test_sentences = test_df[\"text\"].to_list()\n","test_samples = random.sample(test_sentences, 10)\n","for test_sample in test_samples:\n","  pred_prob = tf.squeeze(model_6.predict([test_sample])) # has to be list\n","  pred = tf.round(pred_prob)\n","  print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n","  print(f\"Text:\\n{test_sample}\\n\")\n","  print(\"----\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAYeILl4WizU","executionInfo":{"status":"ok","timestamp":1658349850335,"user_tz":300,"elapsed":1038,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"4a0ce2e7-c0f1-46c4-f837-3ad6e76a448b"},"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["Pred: 0, Prob: 0.17207098007202148\n","Text:\n","This beautifully rapturous facade that we've poured life into has itself given birth to a deplorable fate our total obliteration.\n","\n","----\n","\n","Pred: 0, Prob: 0.1446925401687622\n","Text:\n","While being held hostage by Tobias Hankel\n","\n","----\n","\n","Pred: 0, Prob: 0.3151742219924927\n","Text:\n","@Schwarzenegger @FoxNews you won't because Dems are focused on flooding our borders with illegal immigrants to add to their voters. #EndofUS\n","\n","----\n","\n","Pred: 1, Prob: 0.8320173621177673\n","Text:\n","'Prompt and utter' mass murder  https://t.co/BRU7t5UzUy\n","\n","----\n","\n","Pred: 1, Prob: 0.6474711894989014\n","Text:\n","I'm havin previous life flashbacks of when i lived in Weimar Berlin. the hustlin life on Unter der Linden before the deluge.\n","\n","----\n","\n","Pred: 1, Prob: 0.9904928803443909\n","Text:\n","@wral there are now 5 fire trucks and 2 ambulances at Wellington ridge lp in Cary. one of the units is on fire. not sure on injuries\n","\n","----\n","\n","Pred: 1, Prob: 0.8003494143486023\n","Text:\n","Great photo by the Tribune's Terrence Antonio James after Green Line derailment (no one hurt fortunately) http://t.co/YFavM641OS\n","\n","----\n","\n","Pred: 0, Prob: 0.027427786961197853\n","Text:\n","I screamed when @g_montani @jessemontani were running to John for 1st place! I'm so happy I love you guys ?? #AmazingRaceCanada\n","\n","----\n","\n","Pred: 0, Prob: 0.17970706522464752\n","Text:\n","Here we go....one more Toss with the Seismic boys! With a stellar undercard all killer no filler keep an eye... http://t.co/Xs6aa2c6Jb\n","\n","----\n","\n","Pred: 0, Prob: 0.03220744431018829\n","Text:\n","Blonde beauty Amanda shows you her big natural boobs in her first set http://t.co/qew4c5M1xd View and download video\n","\n","----\n","\n"]}]},{"cell_type":"markdown","source":["## Speed-Score tradeoff\n","\n","Just because a model is the best performing, doesn't mean that it is the best model to use. When dealing with large datasets you need to udnerstand that speed may be more important than absolute accuracy/F1-score/etc. "],"metadata":{"id":"F3f6zuzTWonR"}},{"cell_type":"code","source":["# Calculate the time of predictions\n","import time\n","def pred_timer(model, samples):\n","  \"\"\"\n","  Times how long a model takes to make predictions on samples.\n","  \n","  Args:\n","  ----\n","  model = a trained model\n","  sample = a list of samples\n","\n","  Returns:\n","  ----\n","  total_time = total elapsed time for model to make predictions on samples\n","  time_per_pred = time in seconds per single sample\n","  \"\"\"\n","  start_time = time.perf_counter() # get start time\n","  model.predict(samples) # make predictions\n","  end_time = time.perf_counter() # get finish time\n","  total_time = end_time-start_time # calculate how long predictions took to make\n","  time_per_pred = total_time/len(val_sentences) # find prediction time per sample\n","  return total_time, time_per_pred"],"metadata":{"id":"xqU0_MkuXMRN","executionInfo":{"status":"ok","timestamp":1658350014738,"user_tz":300,"elapsed":138,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["# Calculate TF Hub Sentence Encoder prediction times\n","model_6_total_pred_time, model_6_time_per_pred = pred_timer(model_6, val_sentences)\n","model_6_total_pred_time, model_6_time_per_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8BP7BHHXQ-P","executionInfo":{"status":"ok","timestamp":1658350024023,"user_tz":300,"elapsed":485,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"c78893a2-c9a9-4eb0-dcb3-3069df0ef305"},"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.37610312599986173, 0.0004935736561677976)"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["# Calculate Naive Bayes prediction times\n","baseline_total_pred_time, baseline_time_per_pred = pred_timer(model_0, val_sentences)\n","baseline_total_pred_time, baseline_time_per_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9ufJSTWXTKE","executionInfo":{"status":"ok","timestamp":1658350033222,"user_tz":300,"elapsed":138,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"e4a8b228-da4e-4cf0-b09a-9fd982a0eb01"},"execution_count":134,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.022198236999884102, 2.9131544619270474e-05)"]},"metadata":{},"execution_count":134}]},{"cell_type":"markdown","source":["Wow quite a difference! Factor of 10!"],"metadata":{"id":"P_32oxeiXVes"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 7))\n","plt.scatter(baseline_time_per_pred, baseline_results[\"f1\"], label=\"baseline\")\n","plt.scatter(model_6_time_per_pred, model_6_results[\"f1\"], label=\"tf_hub_sentence_encoder\")\n","plt.legend()\n","plt.title(\"F1-score versus time per prediction\")\n","plt.xlabel(\"Time per prediction\")\n","plt.ylabel(\"F1-Score\");"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"4jx2rncZXbW9","executionInfo":{"status":"ok","timestamp":1658350062882,"user_tz":300,"elapsed":481,"user":{"displayName":"Maximilian Scheder-Bieschin","userId":"13808558721261695458"}},"outputId":"d9fe4481-9845-463a-bc27-f0e81afdf473"},"execution_count":135,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["The ideal location would be on the top left. Fast, and high score!"],"metadata":{"id":"LZLxkaC6XcpV"}}],"metadata":{"accelerator":"GPU","colab":{"name":"08_Intro_to_NLP_TensorFlow.ipynb","provenance":[],"collapsed_sections":["DzDVTj9HNPsS","BXbmmnaEPCwR","uj5Bpbz2Q_NS","-Hpfvl-aLsBG","ImeUhFkSXZzn","h9qXA3rkZerb","HMnNQpPxsQZC","oV-T34OIzyX_","BsZ_Q4N9T8fG","iRlfr643Xq9t","TIGHV3vgayBX","jWkLp9mEhr99","BdbSKIIHtMN4","yK2P2cjr-OU-","sCSyLv8P4CJl","4PtPWx604RNB","rzDyGsWuEJee","Oo9laiI4KUwa"],"authorship_tag":"ABX9TyOHxB3AedypVNUHfaqFP11X"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}